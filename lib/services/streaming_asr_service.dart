import 'dart:async';
import 'dart:typed_data';
import 'dart:math' as math;
import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import '../utils/asr_utils.dart';
import 'text_correction_service.dart';

/// æµå¼ASRæœåŠ¡ - ä½¿ç”¨Sherpa-ONNX paraformer-zh-onlineæ¨¡å‹
/// æ”¯æŒä½å»¶è¿Ÿçš„æµå¼ä¸­æ–‡è¯†åˆ«ï¼Œä¼˜åŒ–ç‰ˆæœ¬ + æ™ºèƒ½çº é”™
class StreamingAsrService {
  sherpa_onnx.OnlineRecognizer? _recognizer;
  sherpa_onnx.OnlineStream? _stream;
  bool _isInitialized = false;

  // æµå¼è¯†åˆ«ç»“æœå›è°ƒ
  StreamController<String> _resultController = StreamController<String>.broadcast();
  Stream<String> get resultStream => _resultController.stream;

  // æ–‡æœ¬çº é”™æœåŠ¡
  final TextCorrectionService _correctionService = TextCorrectionService();
  bool _enableCorrection = true; // æ˜¯å¦å¯ç”¨çº é”™

  // ä¼˜åŒ–éŸ³é¢‘ç¼“å†²åŒºç®¡ç†
  List<double> _audioBuffer = [];
  static const int _optimalChunkSize = 3200; // 200ms at 16kHzï¼Œæ›´é€‚åˆä¸­æ–‡è¯­éŸ³
  static const int _minChunkSize = 800;  // 50ms æœ€å°å¤„ç†å•ä½
  static const double _sampleRate = 16000.0;

  // éŸ³é¢‘è´¨é‡ç»Ÿè®¡
  double _audioLevel = 0.0;
  int _silenceFrameCount = 0;
  static const double _silenceThreshold = 0.01; // é™éŸ³é˜ˆå€¼

  // è¯†åˆ«çŠ¶æ€ç®¡ç†
  String _lastPartialResult = '';
  String _lastFinalResult = '';
  String _lastCorrectedResult = ''; // ä¸Šæ¬¡çº é”™åçš„ç»“æœ
  DateTime _lastActivityTime = DateTime.now();

  // çº é”™ç»Ÿè®¡
  int _totalCorrections = 0;
  int _totalRecognitions = 0;

  bool get isInitialized => _isInitialized;
  double get currentAudioLevel => _audioLevel;
  bool get isCorrectionEnabled => _enableCorrection;
  double get correctionRate => _totalRecognitions > 0 ? _totalCorrections / _totalRecognitions : 0.0;

  /// åˆå§‹åŒ–æµå¼ASRæœåŠ¡
  Future<void> init() async {
    try {
      print('[StreamingAsrService] ğŸš€ Initializing optimized streaming ASR...');

      // åˆå§‹åŒ–Sherpa-ONNXç»‘å®š
      sherpa_onnx.initBindings();

      // åˆ›å»ºä¼˜åŒ–çš„æµå¼è¯†åˆ«å™¨é…ç½®
      final config = await _createOptimizedConfig();

      _recognizer = sherpa_onnx.OnlineRecognizer(config);
      print('[StreamingAsrService] âœ… Optimized recognizer created');

      _stream = _recognizer!.createStream();
      print('[StreamingAsrService] âœ… Audio stream created');

      _isInitialized = true;
      print('[StreamingAsrService] ğŸ‰ Optimized streaming ASR initialized successfully');

    } catch (e, stackTrace) {
      print('[StreamingAsrService] âŒ Failed to initialize: $e');
      print('[StreamingAsrService] Stack trace: $stackTrace');
      _isInitialized = false;
      rethrow;
    }
  }

  /// åˆ›å»ºä¼˜åŒ–çš„æµå¼è¯†åˆ«å™¨é…ç½®
  Future<sherpa_onnx.OnlineRecognizerConfig> _createOptimizedConfig() async {
    print('[StreamingAsrService] ğŸ“ Creating optimized streaming config...');

    // è·å–æ¨¡å‹æ–‡ä»¶è·¯å¾„
    final encoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.onnx');
    final decoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.onnx');
    final tokensPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt');

    print('[StreamingAsrService] ğŸ“‚ Model files loaded');

    // åˆ›å»ºparaformeræ¨¡å‹é…ç½®
    final paraformerConfig = sherpa_onnx.OnlineParaformerModelConfig(
      encoder: encoderPath,
      decoder: decoderPath,
    );

    // ä¼˜åŒ–æ¨¡å‹é…ç½® - å¢åŠ çº¿ç¨‹æ•°å’Œä¼˜åŒ–å‚æ•°
    final modelConfig = sherpa_onnx.OnlineModelConfig(
      paraformer: paraformerConfig,
      tokens: tokensPath,
      numThreads: 4, // å¢åŠ åˆ°4ä¸ªçº¿ç¨‹æé«˜å®æ—¶æ€§èƒ½
      provider: "cpu",
      debug: false, // å…³é—­debugä»¥æé«˜æ€§èƒ½
    );

    // ä¼˜åŒ–ç‰¹å¾æå–é…ç½®
    final featConfig = sherpa_onnx.FeatureConfig(
      sampleRate: 16000,
      featureDim: 80,
    );

    // ä¼˜åŒ–ç«¯ç‚¹æ£€æµ‹å‚æ•°ä»¥é€‚åº”ä¸­æ–‡è¯­éŸ³ç‰¹ç‚¹
    final config = sherpa_onnx.OnlineRecognizerConfig(
      model: modelConfig,
      feat: featConfig,
      enableEndpoint: true,
      // è°ƒæ•´ç«¯ç‚¹æ£€æµ‹å‚æ•°ä»¥é€‚åº”ä¸­æ–‡è¯­éŸ³èŠ‚å¥
      rule1MinTrailingSilence: 1.0,  // å‡å°‘é™éŸ³æ£€æµ‹æ—¶é—´
      rule2MinTrailingSilence: 0.8,  // æ›´å¿«å“åº”çŸ­å¥
      rule3MinUtteranceLength: 10.0, // å‡å°‘æœ€å°è¯è¯­é•¿åº¦
      hotwordsFile: '',
      hotwordsScore: 2.0, // æé«˜çƒ­è¯æƒé‡
      maxActivePaths: 8,   // å¢åŠ æœç´¢è·¯å¾„æ•°é‡
    );

    print('[StreamingAsrService] âœ… Optimized config created');
    return config;
  }

  /// éŸ³é¢‘é¢„å¤„ç† - æ·»åŠ éŸ³é¢‘å¢å¼º
  Float32List _preprocessAudio(Float32List audioData) {
    if (audioData.isEmpty) return audioData;

    // è®¡ç®—éŸ³é¢‘èƒ½é‡
    double energy = 0.0;
    for (double sample in audioData) {
      energy += sample * sample;
    }
    _audioLevel = math.sqrt(energy / audioData.length);

    // æ£€æµ‹é™éŸ³
    if (_audioLevel < _silenceThreshold) {
      _silenceFrameCount++;
    } else {
      _silenceFrameCount = 0;
      _lastActivityTime = DateTime.now();
    }

    // éŸ³é¢‘å½’ä¸€åŒ– - ä½†ä¿æŒåŠ¨æ€èŒƒå›´
    if (_audioLevel > 0.1) {
      final normalizedData = Float32List(audioData.length);
      final scaleFactor = 0.8 / _audioLevel; // é€‚åº¦å½’ä¸€åŒ–
      for (int i = 0; i < audioData.length; i++) {
        normalizedData[i] = audioData[i] * scaleFactor;
        // è½¯é™å¹…
        if (normalizedData[i] > 0.95) normalizedData[i] = 0.95;
        if (normalizedData[i] < -0.95) normalizedData[i] = -0.95;
      }
      return normalizedData;
    }

    return audioData;
  }

  /// å¯¹è¯†åˆ«ç»“æœè¿›è¡Œæ™ºèƒ½çº é”™
  String _correctRecognitionResult(String originalText) {
    if (!_enableCorrection || originalText.isEmpty) {
      return originalText;
    }

    _totalRecognitions++;

    // åº”ç”¨æ–‡æœ¬çº é”™
    String correctedText = _correctionService.correctText(originalText);

    // å¦‚æœå‘ç”Ÿäº†çº é”™ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    if (correctedText != originalText) {
      _totalCorrections++;
      print('[StreamingAsrService] ğŸ”§ çº é”™åº”ç”¨: "$originalText" â†’ "$correctedText"');
    }

    return correctedText;
  }

  /// ä¼˜åŒ–çš„éŸ³é¢‘å¤„ç†æ–¹æ³•ï¼ˆå¸¦çº é”™ï¼‰
  Future<String> processAudio(Float32List audioData) async {
    if (!_isInitialized || _stream == null) {
      print('[StreamingAsrService] âš ï¸ Service not initialized');
      return '';
    }

    try {
      // é¢„å¤„ç†éŸ³é¢‘
      final processedAudio = _preprocessAudio(audioData);

      // å°†éŸ³é¢‘æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
      _audioBuffer.addAll(processedAudio);

      String result = '';

      // ä½¿ç”¨æ›´æ™ºèƒ½çš„ç¼“å†²åŒºå¤„ç†
      while (_audioBuffer.length >= _minChunkSize) {
        // ç¡®å®šå¤„ç†å—å¤§å°
        int chunkSize = math.min(_audioBuffer.length, _optimalChunkSize);

        // å¦‚æœæ¥è¿‘æœ€ä¼˜å¤§å°ï¼Œä½¿ç”¨æœ€ä¼˜å¤§å°
        if (_audioBuffer.length >= _optimalChunkSize) {
          chunkSize = _optimalChunkSize;
        }

        final chunk = Float32List.fromList(_audioBuffer.take(chunkSize).toList());
        _audioBuffer.removeRange(0, chunkSize);

        // è¾“å…¥éŸ³é¢‘åˆ°æµå¼è¯†åˆ«å™¨
        _stream!.acceptWaveform(samples: chunk, sampleRate: _sampleRate.toInt());

        // å¤„ç†è¯†åˆ«
        await _processRecognition();

        // è·å–å½“å‰ç»“æœå¹¶åº”ç”¨çº é”™
        final currentResult = _recognizer!.getResult(_stream!);
        if (currentResult.text.isNotEmpty && currentResult.text != _lastPartialResult) {
          // å¯¹éƒ¨åˆ†ç»“æœåº”ç”¨è½»é‡çº§çº é”™ï¼ˆåªçº æ­£æ˜æ˜¾é”™è¯¯ï¼‰
          String correctedResult = _correctRecognitionResult(currentResult.text);
          result = correctedResult;
          _lastPartialResult = currentResult.text; // è®°å½•åŸå§‹ç»“æœ
          _lastCorrectedResult = correctedResult;  // è®°å½•çº é”™ç»“æœ

          print('[StreamingAsrService] ğŸ™ï¸ Partial: $correctedResult (level: ${_audioLevel.toStringAsFixed(3)})');
          _resultController.add(correctedResult);
        }

        // æ£€æŸ¥ç«¯ç‚¹
        if (_recognizer!.isEndpoint(_stream!)) {
          final finalResult = _recognizer!.getResult(_stream!);
          if (finalResult.text.isNotEmpty && finalResult.text != _lastFinalResult) {
            // å¯¹æœ€ç»ˆç»“æœåº”ç”¨å®Œæ•´çº é”™
            String correctedFinalResult = _correctRecognitionResult(finalResult.text);
            result = correctedFinalResult;
            _lastFinalResult = finalResult.text;     // è®°å½•åŸå§‹ç»“æœ
            _lastCorrectedResult = correctedFinalResult; // è®°å½•çº é”™ç»“æœ

            print('[StreamingAsrService] ğŸ Final: $correctedFinalResult');
            _resultController.add(correctedFinalResult);
          }

          // é‡ç½®æµ
          _recognizer!.reset(_stream!);
          _lastPartialResult = '';
        }
      }

      return result;

    } catch (e, stackTrace) {
      print('[StreamingAsrService] âŒ Error processing audio: $e');
      print('[StreamingAsrService] Stack trace: $stackTrace');
      return '';
    }
  }

  /// å¼‚æ­¥å¤„ç†è¯†åˆ«é€»è¾‘
  Future<void> _processRecognition() async {
    try {
      // æ‰¹é‡å¤„ç†å¯ç”¨çš„éŸ³é¢‘æ•°æ®
      int processedFrames = 0;
      while (_recognizer!.isReady(_stream!) && processedFrames < 10) {
        _recognizer!.decode(_stream!);
        processedFrames++;
      }
    } catch (e) {
      print('[StreamingAsrService] âŒ Error in recognition: $e');
    }
  }

  /// ä¼˜åŒ–çš„è¿ç»­éŸ³é¢‘è¾“å…¥æ–¹æ³•ï¼ˆå¸¦çº é”™ï¼‰
  void feedAudio(Float32List audioData) {
    if (!_isInitialized || _stream == null) return;

    try {
      final processedAudio = _preprocessAudio(audioData);
      _stream!.acceptWaveform(samples: processedAudio, sampleRate: _sampleRate.toInt());

      // éé˜»å¡å¤„ç†
      _processRecognitionAsync();

    } catch (e) {
      print('[StreamingAsrService] âŒ Error feeding audio: $e');
    }
  }

  /// å¼‚æ­¥å¤„ç†è¯†åˆ«ï¼ˆéé˜»å¡ï¼Œå¸¦çº é”™ï¼‰
  void _processRecognitionAsync() {
    Future.microtask(() async {
      try {
        await _processRecognition();

        final result = _recognizer!.getResult(_stream!);
        if (result.text.isNotEmpty && result.text != _lastPartialResult) {
          // åº”ç”¨çº é”™
          String correctedResult = _correctRecognitionResult(result.text);
          _lastPartialResult = result.text;
          _lastCorrectedResult = correctedResult;
          _resultController.add(correctedResult);
        }
      } catch (e) {
        print('[StreamingAsrService] âŒ Async recognition error: $e');
      }
    });
  }

  /// è·å–å½“å‰è¯†åˆ«ç»“æœï¼ˆçº é”™åï¼‰
  String getCurrentResult() {
    if (!_isInitialized || _stream == null) return '';

    try {
      final result = _recognizer!.getResult(_stream!);
      return _correctRecognitionResult(result.text);
    } catch (e) {
      print('[StreamingAsrService] âŒ Error getting result: $e');
      return '';
    }
  }

  /// æ™ºèƒ½åˆ·æ–° - æ ¹æ®éŸ³é¢‘çŠ¶æ€å†³å®šæ˜¯å¦å¼ºåˆ¶ç»“æŸï¼ˆå¸¦çº é”™ï¼‰
  String flushAndGetResult() {
    if (!_isInitialized || _stream == null) return '';

    try {
      // æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„éŸ³é¢‘
      if (_audioBuffer.isNotEmpty) {
        // å¤„ç†å‰©ä½™çš„éŸ³é¢‘ç¼“å†²åŒº
        final remaining = Float32List.fromList(_audioBuffer);
        _audioBuffer.clear();
        _stream!.acceptWaveform(samples: remaining, sampleRate: _sampleRate.toInt());
      }

      // æ·»åŠ é€‚é‡é™éŸ³æ¥è§¦å‘ç«¯ç‚¹
      final silenceDuration = _silenceFrameCount > 10 ? 800 : 1600; // æ ¹æ®é™éŸ³æƒ…å†µè°ƒæ•´
      final silence = Float32List(silenceDuration);
      _stream!.acceptWaveform(samples: silence, sampleRate: _sampleRate.toInt());

      // å¤„ç†å‰©ä½™è¯†åˆ«
      while (_recognizer!.isReady(_stream!)) {
        _recognizer!.decode(_stream!);
      }

      final result = _recognizer!.getResult(_stream!);
      if (result.text.isNotEmpty) {
        // åº”ç”¨çº é”™
        String correctedResult = _correctRecognitionResult(result.text);
        _resultController.add(correctedResult);
        _recognizer!.reset(_stream!);
        _lastPartialResult = '';
        _lastFinalResult = result.text;
        _lastCorrectedResult = correctedResult;
        print('[StreamingAsrService] ğŸ”„ Flushed result: $correctedResult');
        return correctedResult;
      }

      return '';

    } catch (e) {
      print('[StreamingAsrService] âŒ Error flushing: $e');
      return '';
    }
  }

  /// å¯ç”¨/ç¦ç”¨æ–‡æœ¬çº é”™
  void setTextCorrectionEnabled(bool enabled) {
    _enableCorrection = enabled;
    print('[StreamingAsrService] ${enabled ? 'âœ…' : 'âŒ'} æ–‡æœ¬çº é”™${enabled ? 'å·²å¯ç”¨' : 'å·²ç¦ç”¨'}');
  }

  /// è·å–çº é”™ç»Ÿè®¡ä¿¡æ¯
  Map<String, dynamic> getCorrectionStats() {
    return {
      'totalRecognitions': _totalRecognitions,
      'totalCorrections': _totalCorrections,
      'correctionRate': correctionRate,
      'isEnabled': _enableCorrection,
      'lastOriginal': _lastFinalResult,
      'lastCorrected': _lastCorrectedResult,
      ..._correctionService.getCorrectionStats(),
    };
  }

  /// è·å–è¯†åˆ«çŠ¶æ€ä¿¡æ¯ï¼ˆåŒ…å«çº é”™ä¿¡æ¯ï¼‰
  Map<String, dynamic> getStatus() {
    return {
      'isInitialized': _isInitialized,
      'audioLevel': _audioLevel,
      'silenceFrames': _silenceFrameCount,
      'bufferSize': _audioBuffer.length,
      'lastActivity': _lastActivityTime.millisecondsSinceEpoch,
      'lastPartial': _lastPartialResult,
      'lastFinal': _lastFinalResult,
      'lastCorrected': _lastCorrectedResult,
      'correctionEnabled': _enableCorrection,
      'correctionRate': correctionRate,
    };
  }

  /// é‡ç½®è¯†åˆ«å™¨çŠ¶æ€ï¼ˆåŒ…å«çº é”™ç»Ÿè®¡ï¼‰
  void reset() {
    if (!_isInitialized || _stream == null) return;

    try {
      _recognizer!.reset(_stream!);
      _audioBuffer.clear();
      _lastPartialResult = '';
      _lastFinalResult = '';
      _lastCorrectedResult = '';
      _silenceFrameCount = 0;
      _audioLevel = 0.0;
      print('[StreamingAsrService] ğŸ”„ Recognizer reset');
    } catch (e) {
      print('[StreamingAsrService] âŒ Error resetting: $e');
    }
  }

  /// é‡Šæ”¾èµ„æº
  void dispose() {
    try {
      print('[StreamingAsrService] ğŸ§¹ Disposing resources...');

      _stream?.free();
      _stream = null;

      _recognizer?.free();
      _recognizer = null;

      _audioBuffer.clear();
      _resultController.close();

      _isInitialized = false;
      print('[StreamingAsrService] âœ… Resources disposed');

    } catch (e) {
      print('[StreamingAsrService] âŒ Error disposing: $e');
    }
  }
}
