import 'dart:async';
import 'dart:typed_data';
import 'package:flutter/services.dart';
import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import '../utils/asr_utils.dart';

/// æµå¼ASRæœåŠ¡ - ä½¿ç”¨Sherpa-ONNX paraformer-zh-onlineæ¨¡å‹
/// æ”¯æŒä½å»¶è¿Ÿçš„æµå¼ä¸­æ–‡è¯†åˆ«
class StreamingAsrService {
  sherpa_onnx.OnlineRecognizer? _recognizer;
  sherpa_onnx.OnlineStream? _stream;
  bool _isInitialized = false;

  // æµå¼è¯†åˆ«ç»“æœå›è°ƒ
  StreamController<String> _resultController = StreamController<String>.broadcast();
  Stream<String> get resultStream => _resultController.stream;

  // ç”¨äºç´¯ç§¯éŸ³é¢‘æ•°æ®çš„ç¼“å†²åŒº
  List<double> _audioBuffer = [];
  static const int _bufferSize = 1600; // 100ms at 16kHz

  bool get isInitialized => _isInitialized;

  /// åˆå§‹åŒ–æµå¼ASRæœåŠ¡
  Future<void> init() async {
    try {
      print('[StreamingAsrService] ğŸš€ Initializing streaming ASR with paraformer...');

      // åˆå§‹åŒ–Sherpa-ONNXç»‘å®š
      sherpa_onnx.initBindings();

      // åˆ›å»ºæµå¼è¯†åˆ«å™¨é…ç½®
      final config = await _createStreamingConfig();

      // ä¿®å¤APIè°ƒç”¨ï¼šä½¿ç”¨æ­£ç¡®çš„æ„é€ å‡½æ•°è¯­æ³•
      _recognizer = sherpa_onnx.OnlineRecognizer(config);
      print('[StreamingAsrService] âœ… Online recognizer created');

      // åˆ›å»ºæµå¼éŸ³é¢‘æµ
      _stream = _recognizer!.createStream();
      print('[StreamingAsrService] âœ… Audio stream created');

      _isInitialized = true;
      print('[StreamingAsrService] ğŸ‰ Streaming ASR initialized successfully');

    } catch (e, stackTrace) {
      print('[StreamingAsrService] âŒ Failed to initialize: $e');
      print('[StreamingAsrService] Stack trace: $stackTrace');
      _isInitialized = false;
      rethrow;
    }
  }

  /// åˆ›å»ºæµå¼è¯†åˆ«å™¨é…ç½®
  Future<sherpa_onnx.OnlineRecognizerConfig> _createStreamingConfig() async {
    print('[StreamingAsrService] ğŸ“ Creating streaming config...');

    // è·å–æ¨¡å‹æ–‡ä»¶è·¯å¾„
    final encoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.onnx');
    final decoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.onnx');
    final tokensPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt');

    print('[StreamingAsrService] ğŸ“‚ Model files copied:');
    print('[StreamingAsrService]   Encoder: $encoderPath');
    print('[StreamingAsrService]   Decoder: $decoderPath');
    print('[StreamingAsrService]   Tokens: $tokensPath');

    // åˆ›å»ºparaformeræ¨¡å‹é…ç½®
    final paraformerConfig = sherpa_onnx.OnlineParaformerModelConfig(
      encoder: encoderPath,
      decoder: decoderPath,
    );

    // åˆ›å»ºæ¨¡å‹é…ç½®
    final modelConfig = sherpa_onnx.OnlineModelConfig(
      paraformer: paraformerConfig,
      tokens: tokensPath,
      numThreads: 2, // ä½¿ç”¨2ä¸ªçº¿ç¨‹ä»¥å¹³è¡¡æ€§èƒ½å’Œå»¶è¿Ÿ
      provider: "cpu",
      debug: true,
    );

    // åˆ›å»ºç‰¹å¾æå–é…ç½®
    final featConfig = sherpa_onnx.FeatureConfig(
      sampleRate: 16000,
      featureDim: 80,
    );

    // åˆ›å»ºè¯†åˆ«å™¨é…ç½® - ç§»é™¤ä¸å­˜åœ¨çš„å‚æ•°
    final config = sherpa_onnx.OnlineRecognizerConfig(
      model: modelConfig,
      feat: featConfig,
      enableEndpoint: true,
      rule1MinTrailingSilence: 2.4,
      rule2MinTrailingSilence: 1.2,
      rule3MinUtteranceLength: 20.0,
      hotwordsFile: '',
      hotwordsScore: 1.5,
      maxActivePaths: 4,
    );

    print('[StreamingAsrService] âœ… Streaming config created');
    return config;
  }

  /// å¤„ç†éŸ³é¢‘æ•°æ® - æ”¯æŒæµå¼è¯†åˆ«
  Future<String> processAudio(Float32List audioData) async {
    if (!_isInitialized || _stream == null) {
      print('[StreamingAsrService] âš ï¸ Service not initialized');
      return '';
    }

    try {
      // å°†éŸ³é¢‘æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
      _audioBuffer.addAll(audioData);

      String result = '';

      // å½“ç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®æ—¶è¿›è¡Œå¤„ç†
      while (_audioBuffer.length >= _bufferSize) {
        // å–å‡ºä¸€ä¸ªç¼“å†²åŒºå¤§å°çš„æ•°æ®
        final chunk = Float32List.fromList(_audioBuffer.take(_bufferSize).toList());
        _audioBuffer.removeRange(0, _bufferSize);

        // è¾“å…¥éŸ³é¢‘åˆ°æµå¼è¯†åˆ«å™¨
        _stream!.acceptWaveform(samples: chunk, sampleRate: 16000);

        // æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„è¯†åˆ«ç»“æœ
        while (_recognizer!.isReady(_stream!)) {
          _recognizer!.decode(_stream!);
        }

        // è·å–éƒ¨åˆ†è¯†åˆ«ç»“æœï¼ˆä¸ç­‰å¾…å¥å­ç»“æŸï¼‰
        final partialResult = _recognizer!.getResult(_stream!);
        if (partialResult.text.isNotEmpty) {
          result = partialResult.text;
          print('[StreamingAsrService] ğŸ™ï¸ Partial result: $result');

          // å‘é€æµå¼ç»“æœ
          _resultController.add(result);
        }

        // æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°ç«¯ç‚¹ï¼ˆå¥å­ç»“æŸï¼‰
        if (_recognizer!.isEndpoint(_stream!)) {
          final finalResult = _recognizer!.getResult(_stream!);
          if (finalResult.text.isNotEmpty) {
            result = finalResult.text;
            print('[StreamingAsrService] ğŸ Final result: $result');

            // å‘é€æœ€ç»ˆç»“æœ
            _resultController.add(result);
          }

          // é‡ç½®æµä»¥å‡†å¤‡ä¸‹ä¸€ä¸ªè¯è¯­
          _recognizer!.reset(_stream!);
        }
      }

      return result;

    } catch (e, stackTrace) {
      print('[StreamingAsrService] âŒ Error processing audio: $e');
      print('[StreamingAsrService] Stack trace: $stackTrace');
      return '';
    }
  }

  /// è¾“å…¥éŸ³é¢‘æ•°æ®ä½†ä¸ç«‹å³è·å–ç»“æœï¼ˆç”¨äºè¿ç»­æµå¼è¯†åˆ«ï¼‰
  void feedAudio(Float32List audioData) {
    if (!_isInitialized || _stream == null) return;

    try {
      _stream!.acceptWaveform(samples: audioData, sampleRate: 16000);

      // è§£ç å¯ç”¨çš„éŸ³é¢‘
      while (_recognizer!.isReady(_stream!)) {
        _recognizer!.decode(_stream!);
      }

      // è·å–å½“å‰è¯†åˆ«ç»“æœ
      final result = _recognizer!.getResult(_stream!);
      if (result.text.isNotEmpty) {
        _resultController.add(result.text);
      }

    } catch (e) {
      print('[StreamingAsrService] âŒ Error feeding audio: $e');
    }
  }

  /// è·å–å½“å‰è¯†åˆ«ç»“æœ
  String getCurrentResult() {
    if (!_isInitialized || _stream == null) return '';

    try {
      final result = _recognizer!.getResult(_stream!);
      return result.text;
    } catch (e) {
      print('[StreamingAsrService] âŒ Error getting result: $e');
      return '';
    }
  }

  /// å¼ºåˆ¶ç»“æŸå½“å‰è¯è¯­å¹¶è·å–æœ€ç»ˆç»“æœ
  String flushAndGetResult() {
    if (!_isInitialized || _stream == null) return '';

    try {
      // è¾“å…¥ä¸€å°æ®µé™éŸ³æ¥è§¦å‘ç«¯ç‚¹æ£€æµ‹
      final silence = Float32List(1600); // 100ms silence
      _stream!.acceptWaveform(samples: silence, sampleRate: 16000);

      // å¤„ç†å‰©ä½™éŸ³é¢‘
      while (_recognizer!.isReady(_stream!)) {
        _recognizer!.decode(_stream!);
      }

      final result = _recognizer!.getResult(_stream!);
      if (result.text.isNotEmpty) {
        _resultController.add(result.text);
        _recognizer!.reset(_stream!);
      }

      return result.text;

    } catch (e) {
      print('[StreamingAsrService] âŒ Error flushing: $e');
      return '';
    }
  }

  /// é‡ç½®è¯†åˆ«å™¨ï¿½ï¿½æ€
  void reset() {
    if (!_isInitialized || _stream == null) return;

    try {
      _recognizer!.reset(_stream!);
      _audioBuffer.clear();
      print('[StreamingAsrService] ğŸ”„ Recognizer reset');
    } catch (e) {
      print('[StreamingAsrService] âŒ Error resetting: $e');
    }
  }

  /// é‡Šæ”¾èµ„æº
  void dispose() {
    try {
      print('[StreamingAsrService] ğŸ§¹ Disposing resources...');

      _stream?.free();
      _stream = null;

      _recognizer?.free();
      _recognizer = null;

      _audioBuffer.clear();
      _resultController.close();

      _isInitialized = false;
      print('[StreamingAsrService] âœ… Resources disposed');

    } catch (e) {
      print('[StreamingAsrService] âŒ Error disposing: $e');
    }
  }
}
