import 'dart:async';
import 'dart:typed_data';
import 'dart:math' as math;
import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import '../utils/asr_utils.dart';
import 'text_correction_service.dart';

/// æµå¼ASRæœåŠ¡ - ä½¿ç”¨Sherpa-ONNX paraformer-zh-onlineæ¨¡å‹
/// æ”¯æŒä½å»¶è¿Ÿçš„æµå¼ä¸­æ–‡è¯†åˆ«ï¼Œä¼˜åŒ–ç‰ˆæœ¬ + æ™ºèƒ½çº é”™
class StreamingAsrService {
  sherpa_onnx.OnlineRecognizer? _recognizer;
  sherpa_onnx.OnlineStream? _stream;
  bool _isInitialized = false;

  // æµå¼è¯†åˆ«ç»“æœå›è°ƒ
  StreamController<String> _resultController = StreamController<String>.broadcast();
  Stream<String> get resultStream => _resultController.stream;

  // æ–‡æœ¬çº é”™æœåŠ¡
  final TextCorrectionService _correctionService = TextCorrectionService();
  bool _enableCorrection = true; // æ˜¯å¦å¯ç”¨çº é”™

  // ä¼˜åŒ–éŸ³é¢‘ç¼“å†²åŒºç®¡ç† - ä½¿ç”¨æ›´é«˜æ•ˆçš„ç¼“å†²åŒº
  Float32List _audioBuffer = Float32List(0);
  int _bufferLength = 0;
  static const int _optimalChunkSize = 800; // ğŸ”§ FIX: é™ä½åˆ°50msï¼ŒåŠ å¿«åˆå§‹å“åº”
  static const int _minChunkSize = 400;  // ğŸ”§ FIX: 25ms æœ€å°å¤„ç†å•ä½ï¼Œæ›´å¿«å¯åŠ¨
  static const double _sampleRate = 16000.0;
  static const int _maxBufferSize = 16000; // 1ç§’æœ€å¤§ç¼“å†²

  // ğŸ”§ FIX: æ·»åŠ å³æ—¶å¤„ç†æ¨¡å¼
  bool _enableInstantProcessing = true; // å¯ç”¨å³æ—¶å¤„ç†ï¼Œä¸ç­‰ç¼“å†²åŒºå¡«æ»¡
  int _processedSamples = 0; // è·Ÿè¸ªå·²å¤„ç†çš„æ ·æœ¬æ•°
  int _totalAudioReceived = 0; // ğŸ”§ NEW: è·Ÿè¸ªæ€»æ¥æ”¶çš„éŸ³é¢‘æ•°æ®é‡
  DateTime _startTime = DateTime.now(); // ğŸ”§ NEW: è®°å½•å¯åŠ¨æ—¶é—´

  // éŸ³é¢‘è´¨é‡ç»Ÿè®¡ - ç®€åŒ–è®¡ç®—
  double _audioLevel = 0.0;
  int _silenceFrameCount = 0;
  static const double _silenceThreshold = 0.01;
  int _frameCounter = 0; // ç”¨äºè·³å¸§ä¼˜åŒ–

  // è¯†åˆ«çŠ¶æ€ç®¡ç†
  String _lastPartialResult = '';
  String _lastFinalResult = '';
  String _lastCorrectedResult = '';
  DateTime _lastActivityTime = DateTime.now();

  // æ€§èƒ½ä¼˜åŒ–å¼€å…³
  bool _enableAudioEnhancement = false; // é»˜è®¤å…³é—­éŸ³é¢‘å¢å¼ºä»¥æå‡é€Ÿåº¦
  bool _enablePartialCorrection = false; // åªå¯¹æœ€ç»ˆç»“æœçº é”™

  // çº é”™ç»Ÿè®¡
  int _totalCorrections = 0;
  int _totalRecognitions = 0;

  bool get isInitialized => _isInitialized;
  double get currentAudioLevel => _audioLevel;
  bool get isCorrectionEnabled => _enableCorrection;
  double get correctionRate => _totalRecognitions > 0 ? _totalCorrections / _totalRecognitions : 0.0;

  /// åˆå§‹åŒ–æµå¼ASRæœåŠ¡
  Future<void> init() async {
    try {
      print('[StreamingAsrService] ğŸš€ Initializing optimized streaming ASR...');

      // åˆå§‹åŒ–Sherpa-ONNXç»‘å®š
      sherpa_onnx.initBindings();

      // åˆ›å»ºä¼˜åŒ–çš„æµå¼è¯†åˆ«å™¨é…ç½®
      final config = await _createOptimizedConfig();

      _recognizer = sherpa_onnx.OnlineRecognizer(config);
      print('[StreamingAsrService] âœ… Optimized recognizer created');

      _stream = _recognizer!.createStream();
      print('[StreamingAsrService] âœ… Audio stream created');

      // é¢„åˆ†é…ç¼“å†²åŒº
      _audioBuffer = Float32List(_maxBufferSize);
      _bufferLength = 0;

      // ğŸ”§ FIX: é‡ç½®å³æ—¶å¤„ç†æ¨¡å¼çŠ¶æ€
      _enableInstantProcessing = true;
      _processedSamples = 0;
      _totalAudioReceived = 0;
      _startTime = DateTime.now();
      
      _isInitialized = true;
      print('[StreamingAsrService] ğŸ‰ Optimized streaming ASR initialized successfully');

    } catch (e, stackTrace) {
      print('[StreamingAsrService] âŒ Failed to initialize: $e');
      print('[StreamingAsrService] Stack trace: $stackTrace');
      _isInitialized = false;
      rethrow;
    }
  }

  /// åˆ›å»ºä¼˜åŒ–çš„æµå¼è¯†åˆ«å™¨é…ç½®
  Future<sherpa_onnx.OnlineRecognizerConfig> _createOptimizedConfig() async {
    print('[StreamingAsrService] ğŸ“ Creating optimized streaming config...');

    // ä¼˜å…ˆä½¿ç”¨int8é‡åŒ–æ¨¡å‹ä»¥æå‡é€Ÿåº¦
    String encoderPath;
    String decoderPath;

    try {
      encoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.int8.onnx');
      decoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.int8.onnx');
      print('[StreamingAsrService] âœ… Using int8 quantized models for better speed');
    } catch (e) {
      // é™çº§åˆ°fp32æ¨¡å‹
      encoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/encoder.onnx');
      decoderPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/decoder.onnx');
      print('[StreamingAsrService] âš ï¸ Using fp32 models, int8 not available');
    }

    final tokensPath = await copyAssetFile('assets/sherpa-onnx-streaming-paraformer-bilingual-zh-en/tokens.txt');

    print('[StreamingAsrService] ğŸ“‚ Model files loaded');

    // åˆ›å»ºparaformeræ¨¡å‹é…ç½®
    final paraformerConfig = sherpa_onnx.OnlineParaformerModelConfig(
      encoder: encoderPath,
      decoder: decoderPath,
    );

    // å¹³è¡¡æ€§èƒ½é…ç½® - é€‚åº¦å¢åŠ çº¿ç¨‹ä½†é¿å…è¿‡åº¦
    final modelConfig = sherpa_onnx.OnlineModelConfig(
      paraformer: paraformerConfig,
      tokens: tokensPath,
      numThreads: 3, // é™ä½åˆ°3ä¸ªçº¿ç¨‹ï¼Œé¿å…ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€
      provider: "cpu",
      debug: false,
    );

    // ä¼˜åŒ–ç‰¹å¾æå–é…ç½®
    final featConfig = sherpa_onnx.FeatureConfig(
      sampleRate: 16000,
      featureDim: 80,
    );

    // ä¼˜åŒ–ç«¯ç‚¹æ£€æµ‹å‚æ•°ä»¥å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§
    final config = sherpa_onnx.OnlineRecognizerConfig(
      model: modelConfig,
      feat: featConfig,
      enableEndpoint: true,
      // è°ƒæ•´ç«¯ç‚¹æ£€æµ‹å‚æ•°ä»¥è·å¾—æ›´å¥½çš„é€Ÿåº¦-å‡†ç¡®æ€§å¹³è¡¡
      rule1MinTrailingSilence: 1.2,  // ç¨å¾®å¢åŠ ä»¥ç¡®ä¿å®Œæ•´æ€§
      rule2MinTrailingSilence: 0.6,  // é™ä½ä»¥æå‡å“åº”é€Ÿåº¦
      rule3MinUtteranceLength: 8.0,  // é™ä½æœ€å°è¯è¯­é•¿åº¦
      hotwordsFile: '',
      hotwordsScore: 2.0,
      maxActivePaths: 6,   // å‡å°‘åˆ°6ä¸ªè·¯å¾„ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œé€Ÿåº¦
    );

    print('[StreamingAsrService] âœ… Optimized config created');
    return config;
  }

  /// è½»é‡çº§éŸ³é¢‘é¢„å¤„ç† - åªåœ¨å¿…è¦æ—¶å¢å¼º
  Float32List _preprocessAudioLight(Float32List audioData) {
    if (audioData.isEmpty) return audioData;

    // è·³å¸§ä¼˜åŒ– - æ¯5å¸§æ‰è®¡ç®—ä¸€æ¬¡éŸ³é¢‘è´¨é‡
    _frameCounter++;
    if (_frameCounter % 5 == 0) {
      // å¿«é€Ÿè®¡ç®—éŸ³é¢‘èƒ½é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
      double energy = 0.0;
      final step = math.max(1, audioData.length ~/ 100); // é‡‡æ ·è®¡ç®—
      for (int i = 0; i < audioData.length; i += step) {
        energy += audioData[i] * audioData[i];
      }
      _audioLevel = math.sqrt(energy / (audioData.length / step));

      // æ£€æµ‹é™éŸ³
      if (_audioLevel < _silenceThreshold) {
        _silenceFrameCount++;
      } else {
        _silenceFrameCount = 0;
        _lastActivityTime = DateTime.now();
      }
    }

    // åªåœ¨å¯ç”¨éŸ³é¢‘å¢å¼ºä¸”éŸ³é¢‘è´¨é‡å·®æ—¶å¤„ç†
    if (_enableAudioEnhancement && _audioLevel > 0.15) {
      return _enhanceAudio(audioData);
    }

    return audioData;
  }

  /// éŸ³é¢‘å¢å¼ºï¼ˆä»…åœ¨éœ€è¦æ—¶è°ƒç”¨ï¼‰
  Float32List _enhanceAudio(Float32List audioData) {
    final normalizedData = Float32List(audioData.length);
    final scaleFactor = 0.7 / _audioLevel; // ä¿å®ˆçš„å½’ä¸€åŒ–

    for (int i = 0; i < audioData.length; i++) {
      normalizedData[i] = (audioData[i] * scaleFactor).clamp(-0.9, 0.9);
    }

    return normalizedData;
  }

  /// æ™ºèƒ½çº é”™ - åªå¯¹æœ€ç»ˆç»“æœè¿›è¡Œå®Œæ•´çº é”™
  String _correctRecognitionResult(String originalText, {bool isFinal = false}) {
    if (!_enableCorrection || originalText.isEmpty) {
      return originalText;
    }

    _totalRecognitions++;

    // å¯¹äºéƒ¨åˆ†ç»“æœï¼Œåªåšå¿«é€Ÿçº é”™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if (!isFinal && !_enablePartialCorrection) {
      return originalText;
    }

    // åº”ç”¨æ–‡æœ¬çº é”™
    String correctedText = _correctionService.correctText(originalText);

    // å¦‚æœå‘ç”Ÿäº†çº é”™ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    if (correctedText != originalText) {
      _totalCorrections++;
      if (isFinal) {
        print('[StreamingAsrService] ï¿½ï¿½ æœ€ï¿½ï¿½ï¿½çº é”™: "$originalText" â†’ "$correctedText"');
      }
    }

    return correctedText;
  }

  /// é«˜æ•ˆçš„ç¼“å†²åŒºç®¡ç†
  void _addToBuffer(Float32List audioData) {
    final dataLength = audioData.length;

    // æ£€æŸ¥ç¼“å†²åŒºç©ºé—´
    if (_bufferLength + dataLength > _audioBuffer.length) {
      // å¦‚æœè¶…å‡ºæœ€å¤§ç¼“å†²åŒºï¼Œç§»é™¤æ—§æ•°æ®
      if (_bufferLength > _maxBufferSize ~/ 2) {
        final keepLength = _maxBufferSize ~/ 2;
        _audioBuffer.setRange(0, keepLength, _audioBuffer, _bufferLength - keepLength);
        _bufferLength = keepLength;
      } else {
        // æ‰©å±•ç¼“å†²åŒº
        final newBuffer = Float32List(_audioBuffer.length * 2);
        newBuffer.setRange(0, _bufferLength, _audioBuffer);
        _audioBuffer = newBuffer;
      }
    }

    // æ·»åŠ æ–°æ•°æ®
    _audioBuffer.setRange(_bufferLength, _bufferLength + dataLength, audioData);
    _bufferLength += dataLength;
  }

  /// ä¼˜åŒ–çš„éŸ³é¢‘å¤„ç†æ–¹æ³•ï¼ˆéé˜»å¡ç‰ˆæœ¬ï¼‰
  Future<String> processAudio(Float32List audioData) async {
    if (!_isInitialized || _stream == null) {
      return '';
    }

    try {
      // è½»é‡çº§é¢„å¤„ç†
      final processedAudio = _preprocessAudioLight(audioData);

      // ğŸ”§ FIX: è·Ÿè¸ªæ€»æ¥æ”¶çš„éŸ³é¢‘æ•°æ®
      _totalAudioReceived += processedAudio.length;

      // é«˜æ•ˆæ·»åŠ åˆ°ç¼“å†²åŒº
      _addToBuffer(processedAudio);

      String result = '';

      // ğŸ”§ FIX: æ”¹è¿›çš„å³æ—¶å¤„ç†æ¨¡å¼åˆ‡æ¢é€»è¾‘
      int targetChunkSize = _optimalChunkSize;
      bool shouldSwitchToNormal = false;
      
      if (_enableInstantProcessing) {
        // å¤šæ¡ä»¶åˆ¤æ–­æ˜¯å¦åº”è¯¥ï¿½ï¿½æ¢åˆ°æ­£å¸¸æ¨¡å¼ï¼š
        // 1. å·²æ¥æ”¶è¶³å¤Ÿçš„éŸ³é¢‘æ•°æ® (1ç§’)
        // 2. æˆ–è€…è¿è¡Œæ—¶é—´è¶…è¿‡3ç§’
        // 3. æˆ–è€…å·²ç»æœ‰è¯†åˆ«ç»“æœè¾“å‡º
        final elapsedMs = DateTime.now().difference(_startTime).inMilliseconds;
        
        if (_totalAudioReceived >= 16000 || // 1ç§’çš„éŸ³é¢‘æ•°æ®
            elapsedMs >= 3000 || // 3ç§’è¿è¡Œæ—¶é—´
            _lastPartialResult.isNotEmpty) { // å·²æœ‰è¯†åˆ«ç»“æœ
          shouldSwitchToNormal = true;
        } else {
          targetChunkSize = _minChunkSize; // ç»§ç»­ä½¿ç”¨å°å—
        }
      }

      // å¤„ç†å®Œæ•´çš„éŸ³é¢‘å—
      while (_bufferLength >= targetChunkSize) {
        // æå–éŸ³é¢‘å—ï¼ˆé¿å…é¢å¤–çš„å†…å­˜åˆ†é…ï¼‰
        final chunk = _audioBuffer.sublist(0, targetChunkSize);

        // ç§»åŠ¨å‰©ä½™æ•°æ®ï¼ˆé«˜æ•ˆçš„å†…å­˜æ“ä½œï¼‰
        final remaining = _bufferLength - targetChunkSize;
        if (remaining > 0) {
          _audioBuffer.setRange(0, remaining, _audioBuffer, targetChunkSize);
        }
        _bufferLength = remaining;

        // ğŸ”§ FIX: å³æ—¶è¾“å…¥éŸ³é¢‘åˆ°è¯†åˆ«å™¨ï¼Œæ— éœ€ç­‰å¾…
        _stream!.acceptWaveform(samples: chunk, sampleRate: _sampleRate.toInt());
        _processedSamples += targetChunkSize;

        // å¼‚æ­¥å¤„ç†è¯†åˆ«ï¼ˆä¸ç­‰å¾…ï¼‰
        _processRecognitionNonBlocking();

        // å¿«é€Ÿæ£€æŸ¥ç»“æœ
        final currentResult = _recognizer!.getResult(_stream!);
        if (currentResult.text.isNotEmpty && currentResult.text != _lastPartialResult) {
          // å¯¹éƒ¨åˆ†ç»“æœåªåšè½»é‡çº é”™
          String correctedResult = _correctRecognitionResult(currentResult.text, isFinal: false);
          result = correctedResult;
          _lastPartialResult = currentResult.text;
          _lastCorrectedResult = correctedResult;

          _resultController.add(correctedResult);
          
          // ğŸ”§ FIX: ä¸€æ—¦æœ‰è¯†åˆ«ç»“æœï¼Œç«‹å³åˆ‡æ¢åˆ°æ­£å¸¸æ¨¡å¼
          if (_enableInstantProcessing) {
            shouldSwitchToNormal = true;
          }
        }

        // æ£€æŸ¥ç«¯ç‚¹
        if (_recognizer!.isEndpoint(_stream!)) {
          final finalResult = _recognizer!.getResult(_stream!);
          if (finalResult.text.isNotEmpty && finalResult.text != _lastFinalResult) {
            // å¯¹æœ€ç»ˆç»“æœè¿›è¡Œå®Œæ•´çº é”™
            String correctedFinalResult = _correctRecognitionResult(finalResult.text, isFinal: true);
            result = correctedFinalResult;
            _lastFinalResult = finalResult.text;
            _lastCorrectedResult = correctedFinalResult;

            print('[StreamingAsrService] ğŸ Final: $correctedFinalResult');
            _resultController.add(correctedFinalResult);
          }

          // é‡ç½®æµ
          _recognizer!.reset(_stream!);
          _lastPartialResult = '';
        }

        // ğŸ”§ FIX: æ‰§è¡Œæ¨¡å¼åˆ‡æ¢
        if (shouldSwitchToNormal && _enableInstantProcessing) {
          _enableInstantProcessing = false;
          targetChunkSize = _optimalChunkSize; // ç«‹å³åˆ‡æ¢åˆ°æ­£å¸¸å—å¤§å°
          print('[StreamingAsrService] ğŸš€ åˆ‡æ¢åˆ°æ­£å¸¸å¤„ç†æ¨¡å¼ (éŸ³é¢‘:${_totalAudioReceived}, æ—¶é—´:${DateTime.now().difference(_startTime).inMilliseconds}ms, ç»“æœ:${_lastPartialResult.isNotEmpty})');
        }
      }

      return result;

    } catch (e) {
      print('[StreamingAsrService] âŒ Error processing audio: $e');
      return '';
    }
  }

  /// éé˜»å¡è¯†åˆ«å¤„ç†
  void _processRecognitionNonBlocking() {
    try {
      // é™åˆ¶å¤„ç†å¸§æ•°ä»¥é¿å…é˜»å¡
      int processedFrames = 0;
      while (_recognizer!.isReady(_stream!) && processedFrames < 5) {
        _recognizer!.decode(_stream!);
        processedFrames++;
      }
    } catch (e) {
      print('[StreamingAsrService] âŒ Error in non-blocking recognition: $e');
    }
  }

  /// ä¼˜åŒ–çš„è¿ç»­éŸ³é¢‘è¾“å…¥æ–¹æ³•
  void feedAudio(Float32List audioData) {
    if (!_isInitialized || _stream == null) return;

    try {
      final processedAudio = _preprocessAudioLight(audioData);
      _stream!.acceptWaveform(samples: processedAudio, sampleRate: _sampleRate.toInt());

      // å®Œå…¨éé˜»å¡å¤„ç†
      Future.microtask(() => _processRecognitionNonBlocking());

    } catch (e) {
      print('[StreamingAsrService] âŒ Error feeding audio: $e');
    }
  }

  /// è·å–å½“å‰è¯†åˆ«ç»“æœï¼ˆçº é”™åï¼‰
  String getCurrentResult() {
    if (!_isInitialized || _stream == null) return '';

    try {
      final result = _recognizer!.getResult(_stream!);
      return _correctRecognitionResult(result.text);
    } catch (e) {
      print('[StreamingAsrService] âŒ Error getting result: $e');
      return '';
    }
  }

  /// æ™ºèƒ½åˆ·æ–° - æ ¹æ®éŸ³é¢‘çŠ¶æ€å†³å®šæ˜¯å¦å¼ºåˆ¶ç»“æŸï¼ˆå¸¦çº é”™ï¼‰
  String flushAndGetResult() {
    if (!_isInitialized || _stream == null) return '';

    try {
      // æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„éŸ³é¢‘
      if (_audioBuffer.isNotEmpty) {
        // å¤„ç†å‰©ä½™çš„éŸ³é¢‘ç¼“å†²åŒº
        final remaining = Float32List.fromList(_audioBuffer);
        _audioBuffer.clear();
        _stream!.acceptWaveform(samples: remaining, sampleRate: _sampleRate.toInt());
      }

      // æ·»åŠ é€‚é‡é™ï¿½ï¿½æ¥è§¦å‘ç«¯ç‚¹
      final silenceDuration = _silenceFrameCount > 10 ? 800 : 1600; // æ ¹æ®é™éŸ³æƒ…å†µè°ƒæ•´
      final silence = Float32List(silenceDuration);
      _stream!.acceptWaveform(samples: silence, sampleRate: _sampleRate.toInt());

      // å¤„ç†å‰©ä½™è¯†åˆ«
      while (_recognizer!.isReady(_stream!)) {
        _recognizer!.decode(_stream!);
      }

      final result = _recognizer!.getResult(_stream!);
      if (result.text.isNotEmpty) {
        // åº”ç”¨çº é”™
        String correctedResult = _correctRecognitionResult(result.text);
        _resultController.add(correctedResult);
        _recognizer!.reset(_stream!);
        _lastPartialResult = '';
        _lastFinalResult = result.text;
        _lastCorrectedResult = correctedResult;
        print('[StreamingAsrService] ğŸ”„ Flushed result: $correctedResult');
        return correctedResult;
      }

      return '';

    } catch (e) {
      print('[StreamingAsrService] âŒ Error flushing: $e');
      return '';
    }
  }

  /// å¯ç”¨/ç¦ç”¨æ–‡æœ¬çº é”™
  void setTextCorrectionEnabled(bool enabled) {
    _enableCorrection = enabled;
    print('[StreamingAsrService] ${enabled ? 'âœ…' : 'âŒ'} æ–‡æœ¬çº é”™${enabled ? 'å·²å¯ç”¨' : 'å·²ç¦ç”¨'}');
  }

  /// è·å–çº é”™ç»Ÿè®¡ä¿¡æ¯
  Map<String, dynamic> getCorrectionStats() {
    return {
      'totalRecognitions': _totalRecognitions,
      'totalCorrections': _totalCorrections,
      'correctionRate': correctionRate,
      'isEnabled': _enableCorrection,
      'lastOriginal': _lastFinalResult,
      'lastCorrected': _lastCorrectedResult,
      ..._correctionService.getCorrectionStats(),
    };
  }

  /// è·å–è¯†åˆ«çŠ¶æ€ä¿¡æ¯ï¼ˆåŒ…å«çº é”™ä¿¡æ¯ï¼‰
  Map<String, dynamic> getStatus() {
    return {
      'isInitialized': _isInitialized,
      'audioLevel': _audioLevel,
      'silenceFrames': _silenceFrameCount,
      'bufferSize': _audioBuffer.length,
      'lastActivity': _lastActivityTime.millisecondsSinceEpoch,
      'lastPartial': _lastPartialResult,
      'lastFinal': _lastFinalResult,
      'lastCorrected': _lastCorrectedResult,
      'correctionEnabled': _enableCorrection,
      'correctionRate': correctionRate,
    };
  }

  /// é‡ç½®è¯†åˆ«å™¨çŠ¶æ€ï¼ˆåŒ…å«çº é”™ç»Ÿè®¡ï¼‰
  void reset() {
    if (!_isInitialized || _stream == null) return;

    try {
      _recognizer!.reset(_stream!);
      _audioBuffer.clear();
      _lastPartialResult = '';
      _lastFinalResult = '';
      _lastCorrectedResult = '';
      _silenceFrameCount = 0;
      _audioLevel = 0.0;
      print('[StreamingAsrService] ğŸ”„ Recognizer reset');
    } catch (e) {
      print('[StreamingAsrService] âŒ Error resetting: $e');
    }
  }

  /// é‡Šæ”¾èµ„æº
  void dispose() {
    try {
      print('[StreamingAsrService] ğŸ§¹ Disposing resources...');

      _stream?.free();
      _stream = null;

      _recognizer?.free();
      _recognizer = null;

      _audioBuffer.clear();
      _resultController.close();

      _isInitialized = false;
      print('[StreamingAsrService] âœ… Resources disposed');

    } catch (e) {
      print('[StreamingAsrService] âŒ Error disposing: $e');
    }
  }

  /// æ€§èƒ½ä¼˜åŒ–è®¾ç½®
  void setPerformanceMode({
    bool enableAudioEnhancement = false,
    bool enablePartialCorrection = false,
    bool enableTextCorrection = true,
  }) {
    _enableAudioEnhancement = enableAudioEnhancement;
    _enablePartialCorrection = enablePartialCorrection;
    _enableCorrection = enableTextCorrection;

    print('[StreamingAsrService] ğŸ›ï¸ æ€§èƒ½æ¨¡å¼è®¾ç½®:');
    print('  éŸ³é¢‘å¢å¼º: ${enableAudioEnhancement ? "å¯ç”¨" : "ç¦ç”¨"}');
    print('  éƒ¨åˆ†ç»“æœçº é”™: ${enablePartialCorrection ? "å¯ç”¨" : "ç¦ç”¨"}');
    print('  æ–‡æœ¬çº é”™: ${enableTextCorrection ? "å¯ç”¨" : "ç¦ç”¨"}');
  }
}
