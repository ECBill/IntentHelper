import 'dart:async';
import 'dart:convert';
import 'dart:io';
import 'dart:typed_data';
import 'dart:math' as math;

import 'package:app/constants/prompt_constants.dart';
import 'package:app/constants/wakeword_constants.dart';
import 'package:app/services/cloud_asr.dart';
import 'package:app/services/cloud_tts.dart';
import 'package:app/services/latency_logger.dart';
import 'package:app/services/streaming_asr_service.dart';
import 'package:app/services/summary.dart';
import 'package:app/utils/text_process_utils.dart';
import 'package:app/utils/asr_utils.dart';
import 'package:app/utils/audio_process_util.dart';
import 'package:app/utils/wav/audio_save_util.dart';
import 'package:app/utils/text_utils.dart';

import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:flutter_foreground_task/flutter_foreground_task.dart';
import 'package:intl/intl.dart';
import 'package:record/record.dart';
import 'package:sherpa_onnx/sherpa_onnx.dart' as sherpa_onnx;
import 'package:flutter_tts/flutter_tts.dart';
import 'package:audioplayers/audioplayers.dart';
import 'package:linalg/linalg.dart';

import '../constants/voice_constants.dart';
import '../constants/record_constants.dart';
import '../models/record_entity.dart';
import '../models/summary_entity.dart';
import '../models/speaker_entity.dart';
import '../services/ble_service.dart';
import '../services/objectbox_service.dart';
import '../services/chat_manager.dart';

const int nDct = 257; // DCTçŸ©é˜µç»´åº¦
const int nPca = 47;  // PCAçŸ©é˜µç»´åº¦
const int nPackageByte = 244; // BLEéŸ³é¢‘åŒ…å­—èŠ‚æ•°
final Float32List silence = Float32List((16000 * 5).toInt()); // 5ç§’é™éŸ³ç¼“å†²åŒº

@pragma('vm:entry-point')
void startRecordService() {
  FlutterForegroundTask.setTaskHandler(RecordServiceHandler());
}

class RecordServiceHandler extends TaskHandler {
  // æ ¸å¿ƒç»„ä»¶
  AudioRecorder _record = AudioRecorder();
  sherpa_onnx.VoiceActivityDetector? _vad;
  sherpa_onnx.SpeakerEmbeddingExtractor? _extractor;
  sherpa_onnx.SpeakerEmbeddingManager? _manager;
  StreamingAsrService _streamingAsr = StreamingAsrService();

  // æœåŠ¡å®ä¾‹
  final ObjectBoxService _objectBoxService = ObjectBoxService();
  final CloudTts _cloudTts = CloudTts();
  final CloudAsr _cloudAsr = CloudAsr();
  final ChatManager _chatManager = ChatManager();
  late FlutterTts _flutterTts;

  // çŠ¶æ€å˜é‡
  bool _inDialogMode = false;
  bool _isUsingCloudServices = true;
  bool _isNeedVoiceprintInit = false;
  bool _isInitialized = false;
  bool _isBoneConductionActive = true;
  bool _onRecording = true;
  bool _onMicrophone = false;
  RecordState? _recordState; // æ·»åŠ ç¼ºå¤±çš„å½•éŸ³çŠ¶æ€å˜é‡

  // å£°çº¹ç›¸å…³
  int currentStep = 0;
  String currentSpeaker = '';

  // æµè®¢é˜…
  StreamSubscription<RecordState>? _recordSub;
  StreamSubscription<Uint8List>? _bleDataSubscription;
  StreamSubscription<Uint8List>? _bleAudioStreamSubscription;
  StreamSubscription? _currentSubscription;
  Stream<Uint8List>? _recordStream;

  // BLEç›¸å…³
  int _lastDataReceivedTimestamp = 0;
  int _boneDataReceivedTimestamp = 0;
  final StreamController<Uint8List> _bleAudioStreamController = StreamController<Uint8List>();

  // éŸ³é¢‘å¤„ç†
  Matrix iDctWeightMatrix = Matrix.fill(nDct, nDct, 0.0);
  Matrix iPcaWeightMatrix = Matrix.fill(nPca, nDct, 0.0);
  List<double> combinedAudio = [];
  bool _lastVadState = false;

  // å¯¹è¯æ€»ç»“ç›¸å…³
  Timer? _summaryTimer;
  int _lastSpeechTimestamp = 0;
  int _currentDialogueCharCount = 0;
  int? _currentDialogueStartTime;

  static const int minCharLimit = 100;
  static const int maxCharLimit = 2000;
  static const String _selectedModel = 'gpt-4o';

  // è¯´è¯äººè¯†åˆ«å†å²
  List<double> _userSimilarityHistory = [];
  List<double> _othersSimilarityHistory = [];

  @override
  Future<void> onStart(DateTime timestamp, TaskStarter starter) async {
    // æœåŠ¡å¯åŠ¨æ—¶åˆå§‹åŒ–å„é¡¹èµ„æº
    print('[onStart] ğŸš€ === FOREGROUND SERVICE STARTING ===');

    try {
      print('[onStart] ğŸ”§ Initializing ObjectBoxService...');
      await ObjectBoxService.initialize();
      print('[onStart] âœ… ObjectBoxService initialized');

      print('[onStart] ğŸ¤– Initializing ChatManager...');
      await _chatManager.init(selectedModel: _selectedModel, systemPrompt: '$systemPromptOfChat\n\n${systemPromptOfScenario['voice']}');
      print('[onStart] âœ… ChatManager initialized');

      print('[onStart] ğŸ“Š Loading matrix data...');
      iDctWeightMatrix = await loadRealMatrixFromJson(
          'assets/idct_weight.json',
          nDct, nDct
      );
      iPcaWeightMatrix = await loadRealMatrixFromJson(
          'assets/ipca_weight.json',
          nPca, nDct
      );
      print('[onStart] âœ… Matrix data loaded');

      print('[onStart] ğŸ¤ Starting recording...');
      await _startRecord();
      print('[onStart] âœ… Recording started');

      print('[onStart] ï¿½ï¿½ï¸ Initializing TTS...');
      await _initTts();
      print('[onStart] âœ… TTS initialized');

      print('[onStart] ğŸ“¡ Initializing BLE...');
      _initBle();
      print('[onStart] âœ… BLE initialized');

      print('[onStart] â˜ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ Initializing cloud services...');
      await _cloudAsr.init();
      await _cloudTts.init();
      print('[onStart] âœ… Cloud services initialized');

      _isUsingCloudServices = _cloudAsr.isAvailable && _cloudTts.isAvailable;
      print('[onStart] ğŸŒ Using cloud services: $_isUsingCloudServices');

      print('[onStart] â° Creating summary timer...');
      _summaryTimer = Timer.periodic(Duration(seconds: 30), (_) {
        print('[onStart] â° Timer triggered - calling _checkAndSummarizeDialogue()');
        _checkAndSummarizeDialogue();
      });
      print('[onStart] âœ… Summary timer created successfully');

      print('[onStart] ğŸ‰ === FOREGROUND SERVICE STARTED SUCCESSFULLY ===');
    } catch (e) {
      print('[onStart] âŒ Error during startup: $e');
      rethrow;
    }
  }

  @override
  void onRepeatEvent(DateTime timestamp) {
    // å®šæ—¶äº‹ä»¶ï¼ˆå¯ç”¨äºå¯¹è¯æ€»ç»“ç­‰ï¼‰
    // Perform conversation summarization repeatedly
    // DialogueSummary.start();
    // TodoManager.start();
  }

  @override
  void onReceiveData(Object data) async {
    // å¤„ç†ä¸»çº¿ç¨‹å‘é€è¿‡æ¥çš„å„ç§æ§åˆ¶ä¿¡å·
    print('[onReceiveData] ğŸ“© Received data: $data');

    if (data == voice_constants.voiceprintDone) {
      print('[onReceiveData] ğŸ Voiceprint done signal received');
      _isNeedVoiceprintInit = false;
    } else if (data == voice_constants.voiceprintStart) {
      print('[onReceiveData] ğŸ—£ï¸ Voiceprint start signal received!');
      _startVoiceprint();
    } else if (data == 'startRecording') {
      print('[onReceiveData] ğŸ¤ Start recording signal received');
      _onRecording = true;
    } else if (data == 'stopRecording') {
      print('[onReceiveData] ğŸ›‘ Stop recording signal received');
      _onRecording = false;
    } else if (data == 'device') {
      print('[onReceiveData] ğŸ“± Device connection signal received');
      var remoteId = await FlutterForegroundTask.getData(key: 'deviceRemoteId');
      if (remoteId != null) {
        await BleService().getAndConnect(remoteId);
        BleService().listenToConnectionState();
      }
    } else if (data == Constants.actionStartMicrophone) {
      print('[onReceiveData] ğŸ™ï¸ Start microphone signal received');
      FlutterForegroundTask.sendDataToMain({
        // 'isMeeting': false, // ç§»é™¤ä¼šè®®æ¨¡å¼
      });
      await _startMicrophone();
    } else if (data == Constants.actionStopMicrophone) {
      print('[onReceiveData] ğŸ™ï¸ Stop microphone signal received');
      await _stopMicrophone();
    }
    FlutterForegroundTask.sendDataToMain(Constants.actionDone);
  }

  @override
  Future<void> onDestroy(DateTime timestamp) async {
    // æœåŠ¡é”€æ¯æ—¶é‡Šæ”¾èµ„æº
    await _stopRecord();
    _bleDataSubscription?.cancel();
    _bleAudioStreamSubscription?.cancel();
    _summaryTimer?.cancel();
    BleService().dispose();
  }

  @override
  void onNotificationButtonPressed(String id) async {
    // å¤„ç†é€šçŸ¥æ æŒ‰é’®ç‚¹å‡»äº‹ä»¶
    if (id == Constants.actionStopRecord) {
      await _stopRecord();
      if (await FlutterForegroundTask.isRunningService) {
        FlutterForegroundTask.stopService();
      }
    }
  }

  // åˆå§‹åŒ–BLEæœåŠ¡ï¼Œç›‘å¬BLEæ•°æ®æµ
  void _initBle() async {
    await BleService().init();
    _bleDataSubscription?.cancel();
    _bleDataSubscription = BleService().dataStream.listen((value) {
      final currentTime = DateTime.now().millisecondsSinceEpoch;

      if (value.length == nPackageByte) {
        if (value[0] == 0xff || value[0] == 0xfe) {
          // ä¼šè®®æ¨¡å¼ç›¸å…³é€»è¾‘ç§»é™¤
          _decodeAndProcessBlePackage(value, currentTime);
        } else if (value[0] == 0x01) {
          if (!_isBoneConductionActive) {
            _isBoneConductionActive = true;
            FlutterForegroundTask.sendDataToMain({'isBoneConductionActive': true});
          }
          _boneDataReceivedTimestamp = currentTime;
        } else if (value[0] == 0x00) {
          if (_isBoneConductionActive && currentTime - _boneDataReceivedTimestamp > 2000) {
            _isBoneConductionActive = false;
            FlutterForegroundTask.sendDataToMain({'isBoneConductionActive': false});
          }
        }
      } else {
        if (kDebugMode) {
          print("Unexpected BLE data length: {value.length}");
        }
      }
    });

    _bleAudioStreamSubscription?.cancel();
    _bleAudioStreamSubscription = _bleAudioStreamController.stream.listen((bleAudioClip) {
      _processAudioData(bleAudioClip);
    });
  }

  // è§£ç å¹¶å¤„ç†BLEéŸ³é¢‘åŒ…
  void _decodeAndProcessBlePackage(Uint8List value, int currentTime) async {
    // ä¼šè®®æ¨¡å¼ç›¸å…³é€»è¾‘ç§»é™¤
    _lastDataReceivedTimestamp = currentTime;
    for (var i = 0; i < 3; i++) {
      var audioSlice = AudioProcessingUtil.processSinglePackage(
          value.sublist(1 + i * 80, 1 + (i + 1) * 80),
          iPcaWeightMatrix,
          iDctWeightMatrix
      );
      combinedAudio.addAll(audioSlice);
      if (combinedAudio.length == 512) {
        Uint8List data = Uint8List(512 * 2);
        for (var j = 0; j < 512; j++) {
          data[j * 2] = (combinedAudio[j] * 32767).toInt();
          data[j * 2 + 1] = (combinedAudio[j] * 32767).toInt() >> 8;
        }
        _bleAudioStreamController.add(data);
        combinedAudio.clear();
      }
    }
  }

  // åˆå§‹åŒ–TTSï¼ˆæœ¬åœ°æˆ–äº‘ç«¯ï¼‰
  Future<void> _initTts() async {
    try {
      print('[_initTts] ğŸ”„ Starting TTS initialization...');

      _flutterTts = FlutterTts();

      if (Platform.isAndroid) {
        await _flutterTts.setQueueMode(1);

        try {
          await _flutterTts.setLanguage("en-US");
          print('[_initTts] ğŸ—£ï¸ Language set to en-US');
        } catch (langError) {
          print('[_initTts] âš ï¿½ï¿½ Failed to set language: $langError');
        }
      }

      await _flutterTts.awaitSpeakCompletion(true);

      print('[_initTts] âœ… TTS initialized successfully');
    } catch (e) {
      print('[_initTts] âŒ TTS initialization error: $e');
      try {
        _flutterTts = FlutterTts();
      } catch (fallbackError) {
        print('[_initTts] âŒ Fallback TTS creation also failed: $fallbackError');
      }
    }
  }

  // åˆå§‹åŒ–ASRï¼ˆVADã€æœ¬åœ°ASRã€å£°çº¹è¯†åˆ«ç­‰ï¼‰
  Future<void> _initAsr() async {
    if (!_isInitialized) {

      sherpa_onnx.initBindings();

      _vad = await initVad();

      // åˆå§‹åŒ–æµå¼ASRæœåŠ¡
      print('[_initAsr] ğŸ¯ Initializing streaming ASR service...');
      await _streamingAsr.init();

      // åº”ç”¨æ€§èƒ½ä¼˜åŒ–è®¾ç½® - ä¼˜å…ˆé€Ÿåº¦ï¼Œä¿æŒå‡†ç¡®æ€§
      _streamingAsr.setPerformanceMode(
        enableAudioEnhancement: false,      // å…³é—­éŸ³é¢‘å¢å¼ºä»¥æå‡é€Ÿåº¦
        enablePartialCorrection: false,     // åªå¯¹æœ€ç»ˆç»“æœçº é”™
        enableTextCorrection: true,         // ä¿æŒçº é”™åŠŸèƒ½ä»¥ç»´æŒå‡†ç¡®æ€§
      );

      print('[_initAsr] âœ… Streaming ASR service initialized with optimized settings');

      await _initSpeakerRecognition();

      _recordSub = _record.onStateChanged().listen((recordState) {
        _recordState = recordState;
      });

      _isInitialized = true;
    }
  }

  // åˆå§‹åŒ–å£°çº¹è¯†åˆ«æ¨¡å‹å’Œç®¡ç†å™¨
  Future<void> _initSpeakerRecognition() async {
    try {
      print('[_initSpeakerRecognition] ğŸ”„ å¼€å§‹åˆå§‹åŒ–å£°çº¹è¯†åˆ«ç³»ç»Ÿ...');

      // å°è¯•è·å–å£°çº¹æ¨¡å‹æ–‡ä»¶
      String? modelPath = await _ensureSpeakerModelExists();

      if (modelPath == null) {
        print('[_initSpeakerRecognition] âŒ æ— æ³•è·å–å£°çº¹æ¨¡å‹æ–‡ä»¶ï¼Œå£°çº¹è¯†åˆ«å°†è¢«ç¦ç”¨');
        _extractor = null;
        _manager = null;
        return;
      }

      print('[_initSpeakerRecognition] ğŸ“ ä½¿ç”¨æ¨¡å‹æ–‡ä»¶: $modelPath');

      // åˆ›å»ºå£°çº¹æå–å™¨é…ç½®
      final config = sherpa_onnx.SpeakerEmbeddingExtractorConfig(model: modelPath);
      _extractor = sherpa_onnx.SpeakerEmbeddingExtractor(config: config);

      if (_extractor == null) {
        print('[_initSpeakerRecognition] âŒ å£°çº¹æå–å™¨åˆ›å»ºå¤±è´¥');
        return;
      }

      // åˆ›å»ºå£°çº¹ç®¡ç†å™¨
      _manager = sherpa_onnx.SpeakerEmbeddingManager(_extractor!.dim);
      print('[_initSpeakerRecognition] âœ… å£°çº¹ç®¡ç†å™¨åˆ›å»ºæˆåŠŸï¼Œç»´åº¦: ${_extractor!.dim}');

      // åŠ è½½å·²æ³¨å†Œçš„ç”¨æˆ·å£°çº¹
      await _loadRegisteredSpeakers();

      print('[_initSpeakerRecognition] âœ… å£°çº¹è¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ');
    } catch (e) {
      print('[_initSpeakerRecognition] âŒ å£°çº¹è¯†åˆ«åˆå§‹åŒ–å¤±è´¥: $e');
      print('[_initSpeakerRecognition] ğŸ”„ å£°çº¹è¯†åˆ«å°†è¢«ç¦ç”¨...');

      // å®Œå…¨ç¦ç”¨å£°çº¹è¯†åˆ«ï¼Œé¿å…åˆ›å»ºè™šæ‹Ÿmanager
      _extractor = null;
      _manager = null;
    }
  }

  // ç¡®ä¿å£°çº¹æ¨¡å‹æ–‡ä»¶å­˜åœ¨
  Future<String?> _ensureSpeakerModelExists() async {
    try {
      // æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„æ¨¡å‹æ–‡ä»¶
      final modelCandidates = [
        'assets/3dspeaker_speech_eres2net_base_200k_sv_zh-cn_16k-common.onnx', // ç”¨æˆ·æ–°ä¸‹è½½çš„3dspeakeræ¨¡å‹ (ç¬¬ä¸€ä¼˜å…ˆçº§)
        'assets/voxceleb_resnet34_LM.onnx',          // WeSpeaker ResNet34 Large-Margin å¤‡ç”¨
        'assets/wespeaker_resnet34.onnx',           // WeSpeaker ResNet34 å¤‡ç”¨
        'assets/speaker_embedding.onnx',             // é€šç”¨å£°çº¹æ¨¡å‹
        'assets/cam++_voxceleb.onnx',               // CAM++ VoxCeleb
      ];

      for (String modelAssetPath in modelCandidates) {
        try {
          final modelPath = await copyAssetFile(modelAssetPath);
          print('[_ensureSpeakerModelExists] âœ… æˆåŠŸåŠ è½½æ¨¡å‹: $modelAssetPath');
          return modelPath;
        } catch (e) {
          print('[_ensureSpeakerModelExists] âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: $modelAssetPath');
          continue;
        }
      }

      print('[_ensureSpeakerModelExists] âŒ æ‰€æœ‰é¢„è®¾æ¨¡å‹æ–‡ä»¶éƒ½ä¸å­˜åœ¨');
      return null;
    } catch (e) {
      print('[_ensureSpeakerModelExists] âŒ è·å–æ¨¡å‹æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: $e');
      return null;
    }
  }

  // ä»ç½‘ç»œä¸‹è½½å£°çº¹æ¨¡å‹ (æš‚æ—¶ä¿ç•™æ¥å£)
  Future<String?> _downloadSpeakerModel() async {
    try {
      print('[_downloadSpeakerModel] ğŸŒ å¼€å§‹ä¸‹è½½å£°çº¹æ¨¡å‹æ–‡ä»¶...');

      // è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ä¸‹è½½é€»è¾‘
      // æ¨èä¸‹è½½åœ°å€ï¼š
      // ECAPA-TDNN: https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb
      // WeSpeaker: https://github.com/wenet-e2e/wespeaker

      print('[_downloadSpeakerModel] âš ï¸ ç½‘ç»œä¸‹è½½åŠŸèƒ½æš‚æœªå®ç°');
      return null;
    } catch (e) {
      print('[_downloadSpeakerModel] âŒ ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¤±è´¥: $e');
      return null;
    }
  }

  // åŠ è½½å·²æ³¨å†Œçš„è¯´è¯äºº
  Future<void> _loadRegisteredSpeakers() async {
    try {
      final speakers = _objectBoxService.getUserSpeaker();
      if (speakers == null || speakers.isEmpty) {
        print('[_loadRegisteredSpeakers] â„¹ï¸ æ²¡æœ‰å·²æ³¨å†Œçš„ç”¨æˆ·å£°çº¹');
        return;
      }

      int loadedCount = 0;
      for (var speaker in speakers) {
        if (speaker.name != null && speaker.embedding != null && speaker.embedding!.isNotEmpty) {
          try {
            _manager!.add(name: speaker.name!, embedding: Float32List.fromList(speaker.embedding!));
            loadedCount++;
            print('[_loadRegisteredSpeakers] âœ… åŠ è½½ç”¨æˆ·å£°çº¹: ${speaker.name}');
          } catch (addError) {
            print('[_loadRegisteredSpeakers] âš ï¸ åŠ è½½ç”¨æˆ·å£°çº¹å¤±è´¥ ${speaker.name}: $addError');
          }
        }
      }

      print('[_loadRegisteredSpeakers] ğŸ“Š æ€»å…±åŠ è½½äº† $loadedCount ä¸ªç”¨æˆ·å£°çº¹');
    } catch (e) {
      print('[_loadRegisteredSpeakers] âŒ åŠ è½½å·²æ³¨å†Œè¯´è¯äººå¤±è´¥: $e');
    }
  }

  // å¯åŠ¨å½•éŸ³æµç¨‹
  Future<void> _startRecord() async {
    await _initAsr();

    // ç§»é™¤è“ç‰™é™åˆ¶ï¼Œç›´æ¥å¯åŠ¨éº¦å…‹é£
    print('[_startRecord] ğŸ¤ Starting microphone recording...');
    _startMicrophone();

    FlutterForegroundTask.saveData(key: 'isRecording', value: true);
    // create stop action button
    FlutterForegroundTask.updateService(
      notificationText: 'Recording...',
      notificationButtons: [
        const NotificationButton(id: Constants.actionStopRecord, text: 'stop'),
      ],
    );
  }

  // å¯åŠ¨éº¦å…‹é£å½•éŸ³
  Future<void> _startMicrophone() async {
    print('[_startMicrophone] ğŸ™ï¸ === MICROPHONE START ATTEMPT ===');
    print('[_startMicrophone] Current state: _onMicrophone = $_onMicrophone');
    print('[_startMicrophone] Current recordStream: ${_recordStream != null ? "EXISTS" : "NULL"}');

    if (_onMicrophone) {
      print('[_startMicrophone] âš ï¸ Microphone already on, returning');
      return;
    }

    if (_recordStream != null) {
      print('[_startMicrophone] âš ï¸ Record stream already exists, returning');
      return;
    }

    _onMicrophone = true;
    print('[_startMicrophone] ğŸ”„ Setting _onMicrophone = true');

    const config = RecordConfig(
        encoder: AudioEncoder.pcm16bits,
        sampleRate: 16000,
        numChannels: 1
    );
    print('[_startMicrophone] ğŸ”§ Created RecordConfig: ${config.toString()}');

    try {
      _record = AudioRecorder();
      print('[_startMicrophone] ğŸ“± AudioRecorder created');

      // å°è¯•ç›´æ¥å¯åŠ¨å½•éŸ³ï¼Œè€Œä¸æ˜¯å…ˆæ£€æŸ¥æƒé™
      print('[_startMicrophone] ğŸš€ Attempting to start audio stream directly...');

      try {
        _recordStream = await _record.startStream(config);
        print('[_startMicrophone] âœ… Audio stream started successfully!');

        _recordStream?.listen(
                (data) {
              // print('[_startMicrophone] ğŸµ Audio data received: ${data.length} bytes');
              _processAudioData(data);
            },
            onError: (error) {
              print('[_startMicrophone] âŒ Audio stream error: $error');
            },
            onDone: () {
              // print('[_startMicrophone] ğŸ Audio stream ended');
            }
        );

        // print('[_startMicrophone] ğŸ¤ === MICROPHONE STARTED SUCCESSFULLY ===');
      } catch (recordError) {
        print('[_startMicrophone] âŒ Failed to start recording: $recordError');

        // å¦‚ï¿½ï¿½ç›´æ¥å¯åŠ¨å¤±è´¥ï¼Œå†æ£€æŸ¥æƒé™
        print('[_startMicrophone] ğŸ” Checking permissions after failure...');
        bool hasPermission = await _record.hasPermission();
        print('[_startMicrophone] ğŸ” Microphone permission check result: $hasPermission');

        if (!hasPermission) {
          print('[_startMicrophone] âŒ No microphone permission! Requesting...');
          hasPermission = await _record.hasPermission();
          print('[_startMicrophone] ğŸ” Permission after request: $hasPermission');
        }

        _onMicrophone = false;
        _recordStream = null;
      }
    } catch (e) {
      print('[_startMicrophone] âŒ Failed to create AudioRecorder: $e');
      _onMicrophone = false;
      _recordStream = null;
    }
  }

  // å¤„ç†éŸ³é¢‘æ•°æ®ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½VADã€ASRã€å£°çº¹è¯†åˆ«ç­‰ä¸»æµç¨‹ï¼‰
  void _processAudioData(data, {String category = RecordEntity.categoryDefault}) async {
    // print('[_processAudioData] ğŸ¤ Received audio data: ${data.length} bytes');

    if (_vad == null || !_streamingAsr.isInitialized) {
      print('[_processAudioData] âŒ VAD is null: ${_vad == null}, Streaming ASR not initialized: ${!_streamingAsr.isInitialized}');
      return;
    }

    FileService.highSaveWav(
      startMeetingTime: null, // ä¼šè®®ç›¸å…³å‚æ•°ç§»é™¤
      onRecording: false, // ä¼šè®®ç›¸å…³å‚æ•°ç§»é™¤
      data: data,
      numChannels: 1,
      sampleRate: 16000,
    );

    if (!_onRecording) {
      print('[_processAudioData] âŒ Recording is disabled (_onRecording = false)');
      return;
    }

    // print('[_processAudioData] ğŸ”„ Converting audio to Float32...');
    final samplesFloat32 = convertBytesToFloat32(Uint8List.fromList(data));
    // print('[_processAudioData] âœ… Converted to ${samplesFloat32.length} float32 samples');

    // print('[_processAudioData] ğŸ¯ Feeding audio to VAD...');
    _vad!.acceptWaveform(samplesFloat32);

    if (_vad!.isDetected() && _isBoneConductionActive && _inDialogMode) {
      print('[_processAudioData] ğŸ”‡ VAD detected speech during dialog mode, stopping TTS...');
      if (_isUsingCloudServices) {
        if (_cloudTts.isPlaying) {
          _cloudTts.stop();
          AudioPlayer().play(AssetSource('audios/interruption.wav'));
        }
      } else {
        _flutterTts.stop();
      }
    }

    final vadDetected = _vad!.isDetected();
    if (vadDetected) {
      print('[_processAudioData] ğŸ—£ï¸ VAD DETECTED SPEECH! Sending to main...');
      FlutterForegroundTask.sendDataToMain({'isVadDetected': true});
    } else {
      // åªåœ¨çŠ¶æ€å˜åŒ–æ—¶æ‰“å°ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
      if (_lastVadState != vadDetected) {
        print('[_processAudioData] ğŸ”‡ VAD: No speech detected');
        _lastVadState = vadDetected;
      }
      FlutterForegroundTask.sendDataToMain({'isVadDetected': false});
    }

    var text = '';
    int segmentCount = 0;

    // print('[_processAudioData] ğŸ“¦ Checking VAD queue... isEmpty: ${_vad!.isEmpty()}');

    while (!_vad!.isEmpty()) {
      segmentCount++;
      // print('[_processAudioData] ğŸµ Processing audio segment #$segmentCount');

      final samples = _vad!.front().samples;
      // print('[_processAudioData] ğŸ“ Segment has ${samples.length} samples (required: ${_vad!.config.sileroVad.windowSize})');

      if (samples.length < _vad!.config.sileroVad.windowSize) {
        print('[_processAudioData] âš ï¸ Segment too short, skipping...');
        break;
      }
      _vad!.pop();

      // print('[_processAudioData] ğŸ”§ Adding silence padding...');
      Float32List paddedSamples = await _addSilencePadding(samples);
      // print('[_processAudioData] âœ… Padded samples: ${paddedSamples.length}');

      var segment = '';
      if (_inDialogMode && _isUsingCloudServices) {
        print('[_processAudioData] â˜ï¸ Using cloud ASR for recognition...');
        segment = await _cloudAsr.recognize(paddedSamples);
      } else {
        // print('[_processAudioData] ğŸ¯ Using streaming paraformer ASR for recognition...');
        segment = await _streamingAsr.processAudio(paddedSamples);
      }

      // print('[_processAudioData] ğŸ“ ASR result: "$segment"');

      segment = segment.replaceFirst('Buddy', 'Buddie').replaceFirst('buddy', 'buddie');
      // print('[_processAudioData] ğŸ”„ After replacement: "$segment"');

      text += segment;
      print('[_processAudioData] ğŸ“‘ Accumulated text: "$text"');

      _processIntermediateResult(segment);

      print('[_processAudioData] ğŸ­ Extracting speaker embedding...');
      final embedding = getSpeakerEmbedding(samples);
      print('[_processAudioData] âœ… Speaker embedding extracted: ${embedding.length}');

      // ä¿®å¤å£°çº¹å½•åˆ¶ï¿½ï¿½è¾‘ï¼šåªæœ‰åœ¨æœ‰æ–‡æœ¬æ—¶æ‰è¿›è¡Œå£°çº¹éªŒè¯
      if (_isNeedVoiceprintInit) {
        print('[_processAudioData] ğŸ—£ï¸ VOICEPRINT MODE: _isNeedVoiceprintInit = true');
        if (text.trim().isNotEmpty) {
          print('[_processAudioData] âœ… Processing voiceprint with text: "$text"');
          _initVoiceprint(text: text, embedding: embedding);
          return;
        } else {
          print('[_processAudioData] âš ï¸ Voiceprint mode but no text yet, continuing...');
        }
      } else {
        print('[_processAudioData] ğŸ‘¤ Normal mode: identifying speaker...');
        // ğŸ”§ FIX: ç®€åŒ–å£°çº¹è¯†åˆ«é€»è¾‘ï¼Œé¿å…é˜»å¡ASR
        try {
          // æ£€æŸ¥å£°çº¹è´¨é‡
          if (!_isEmbeddingQualityGood(embedding)) {
            print('[_processAudioData] âš ï¸ å£°çº¹è´¨é‡ä¸ä½³ï¼Œé»˜è®¤ä¸ºuser');
            currentSpeaker = 'user'; // é»˜è®¤ä¸ºç”¨æˆ·ï¼Œé¿å…é˜»å¡
          } else {
            // ä½¿ç”¨æ”¹è¿›çš„è¯´è¯äººè¯†åˆ«ï¼Œä½†ä¸é˜»å¡ASR
            currentSpeaker = _identifySpeaker(embedding);
            print('[_processAudioData] ğŸ¯ Speaker identified as: $currentSpeaker');
          }
        } catch (speakerError) {
          print('[_processAudioData] âš ï¸ Speaker identification failed: $speakerError, defaulting to user');
          currentSpeaker = 'user'; // è¯†åˆ«å¤±è´¥æ—¶é»˜è®¤ä¸ºç”¨æˆ·
        }
      }
    }

    print('[_processAudioData] ğŸ Processed $segmentCount segments, final text: "$text"');
    print('[_processAudioData] ğŸ“Š Mode check - _isNeedVoiceprintInit: $_isNeedVoiceprintInit, text.isNotEmpty: ${text.isNotEmpty}');

    // åªæœ‰åœ¨éå£°çº¹åˆå§‹åŒ–æ¨¡å¼æˆ–æ²¡æœ‰è·å¾—æ–‡æœ¬æ—¶æ‰ç»§ç»­å¤„ç†
    if (text.isNotEmpty && !_isNeedVoiceprintInit) {
      print('[_processAudioData] âœ… Calling _processFinalResult with text: "$text", speaker: "$currentSpeaker"');
      _processFinalResult(text, currentSpeaker, category: category);
    } else if (text.isEmpty) {
      print('[_processAudioData] â„¹ï¸ No text generated from this audio segment');
    } else if (_isNeedVoiceprintInit) {
      print('[_processAudioData] â„¹ï¸ In voiceprint mode, waiting for more audio...');
    }
  }

  // å¤„ç†ASRä¸­é—´ç»“æœï¼Œå®æ—¶è¿”å›æ–‡æœ¬
  void _processIntermediateResult(String text) {
    if (text.isEmpty) return;
    if (text.trim().isNotEmpty) {
      FlutterForegroundTask.sendDataToMain({
        'text': text,
        'isEndpoint': false,
        'inDialogMode': _inDialogMode,
      });
    }
  }

  // å¤„ç†ASRæœ€ç»ˆç»“æœï¼Œå­˜å‚¨æ–‡æœ¬ã€ç®¡ç†å¯¹è¯çŠ¶æ€
  void _processFinalResult(String text, String speaker, {String category = RecordEntity.categoryDefault, String? operationId}) {
    if (text.isEmpty) return;

    if (!_inDialogMode && speaker == 'user' && wakeword_constants.wakeWordStartDialog.any((keyword) => text.toLowerCase().contains(keyword))) {
      _inDialogMode = true;
    }

    text = text.trim();
    text = TextProcessUtils.removeBracketsContent(text);
    text = TextProcessUtils.clearIfRepeatedMoreThanFiveTimes(text);
    text = text.trim();

    if (text.isEmpty) {
      return;
    }

    // === æ–°å¢ï¼šå¯¹è¯ç»Ÿè®¡é€»è¾‘ ===
    final now = DateTime.now().millisecondsSinceEpoch;
    _lastSpeechTimestamp = now;
    _currentDialogueCharCount += text.length;
    // åªæœ‰åœ¨åˆ†æ®µåæ‰é‡ç½®_startTimeï¼Œæ­£å¸¸è¯­éŸ³è¿›æ¥æ—¶æ‰èµ‹å€¼
    if (_currentDialogueStartTime == null) {
      _currentDialogueStartTime = now;
    }
    // ======================

    if (text.trim().isNotEmpty) {
      FlutterForegroundTask.sendDataToMain({
        'text': text,
        'isEndpoint': true,
        'inDialogMode': _inDialogMode,
        'speaker': speaker,
      });
    }

    // ä¼šè®®ç›¸å…³æ’å…¥é€»è¾‘ç§»é™¤ï¼Œç»Ÿä¸€æ’å…¥é»˜è®¤/å¯¹è¯è®°å½•
    if (speaker != 'user') {
      _objectBoxService.insertDefaultRecord(RecordEntity(role: 'others', content: text));
      _chatManager.addChatSession('others', text);
    } else {
      if (_inDialogMode) {
        _objectBoxService.insertDialogueRecord(RecordEntity(role: 'user', content: text));
        _chatManager.addChatSession('user', text);
        if (wakeword_constants.wakeWordEndDialog.any((keyword) => text.toLowerCase().contains(keyword))) {
          _inDialogMode = false;
          _vad!.clear();
          if (_isUsingCloudServices) {
            _cloudTts.stop();
          } else {
            _flutterTts.stop();
          }
          AudioPlayer().play(AssetSource('audios/beep.wav'));
        }
      } else {
        _objectBoxService.insertDefaultRecord(RecordEntity(role: 'user', content: text));
        _chatManager.addChatSession('user', text);
      }
    }

    if (_inDialogMode) {
      _currentSubscription?.cancel();
      _currentSubscription = _chatManager.createStreamingRequest(text: text).listen((response) {
        final res = jsonDecode(response);
        final content = res['content'] ?? res['delta'];
        final isFinished = res['isFinished'];

        if (operationId != null) {
          LatencyLogger.recordEnd(operationId, phase: 'llm');
        }

        if (text.trim().isNotEmpty) {
          FlutterForegroundTask.sendDataToMain({
            'currentText': text,
            'isFinished': false,
            'content': res['delta'],
          });
        }
        if (!_isUsingCloudServices) {
          _flutterTts.speak(res['delta']);
        }

        if (_isUsingCloudServices) {
          _cloudTts.speak(res['delta'], operationId: operationId);
        }

        if (isFinished) {
          _objectBoxService.insertDialogueRecord(RecordEntity(role: 'assistant', content: content));
          _chatManager.addChatSession('assistant', content);
        }
      });
    }
  }

  // å£°çº¹æ³¨å†Œæµç¨‹
  void _initVoiceprint({
    required String text,
    required dynamic embedding,
  }) {
    print('[_initVoiceprint] ğŸ—£ï¸ å¼€å§‹å£°çº¹æ³¨å†Œæµç¨‹...');
    print('[_initVoiceprint] ğŸ“ æ–‡æœ¬: "$text"');
    print('[_initVoiceprint] ğŸ­ å½“å‰æ­¥éª¤: $currentStep/${wakeword_constants.welcomePhrases.length}');

    FlutterForegroundTask.sendDataToMain({
      'isEndpoint': true,
    });

    // ç¡®ä¿æœ‰å¯ç”¨çš„managerå’Œæœ‰æ•ˆçš„embedding
    if (_manager == null) {
      print('[_initVoiceprint] âŒ å£°çº¹ç®¡ç†å™¨ä¸å¯ç”¨');
      FlutterForegroundTask.sendDataToMain({'status': 'error', 'message': 'å£°çº¹ç³»ç»Ÿæœªåˆå§‹åŒ–'});
      return;
    }

    if (embedding is! Float32List || embedding.isEmpty) {
      print('[_initVoiceprint] âŒ æ— æ•ˆçš„å£°çº¹ç‰¹å¾');
      FlutterForegroundTask.sendDataToMain({'status': 'error', 'message': 'å£°çº¹ç‰¹å¾æ— æ•ˆ'});
      return;
    }

    // æ£€æŸ¥æ­¥éª¤æ˜¯å¦è¶Šç•Œ
    if (currentStep >= wakeword_constants.welcomePhrases.length) {
      print('[_initVoiceprint] âœ… å£°çº¹æ³¨å†Œå·²å®Œæˆ');
      FlutterForegroundTask.sendDataToMain({'status': 'completed'});
      _isNeedVoiceprintInit = false;
      return;
    }

    // è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
    double similarity = calculateEditDistance(text, wakeword_constants.welcomePhrases[currentStep]);
    print('[_initVoiceprint] ğŸ“Š æ–‡æœ¬ç›¸ä¼¼åº¦: $similarity (è¦æ±‚: >= 0.5)');

    if (similarity >= 0.5) {
      try {
        // æˆåŠŸåŒ¹é…ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥
        currentStep++;
        print('[_initVoiceprint] âœ… æ­¥éª¤ ${currentStep-1} å®Œæˆï¿½ï¿½ï¿½ï¿½ï¿½è¿›å…¥æ­¥éª¤ $currentStep');

        // æ¸…ç©ºæ—§çš„ç”¨æˆ·å£°çº¹
        final existingNames = _manager!.allSpeakerNames.toList();
        for (String name in existingNames) {
          if (name == 'user' || name == 'main_user') {
            _manager!.remove(name);
            print('[_initVoiceprint] ğŸ—‘ï¸ ç§»é™¤æ—§å£°çº¹: $name');
          }
        }

        // æ·»åŠ æ–°çš„ç”¨æˆ·å£°çº¹
        _manager!.add(name: 'user', embedding: embedding);
        print('[_initVoiceprint] â• æ·»åŠ ç”¨æˆ·å£°çº¹åˆ°ç®¡ç†å™¨');

        // ä¿å­˜åˆ°æ•°æ®åº“
        _objectBoxService.insertSpeaker(SpeakerEntity(
          name: 'user',
          model: '3dspeaker_eres2net_base_200k', // æ›´æ–°ä¸ºæ–°çš„3dspeakeræ¨¡å‹åç§°
          embedding: embedding.toList()
        ));
        print('[_initVoiceprint] ğŸ’¾ ä¿å­˜å£°çº¹åˆ°æ•°æ®åº“');

        // æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰æ­¥éª¤
        if (currentStep >= wakeword_constants.welcomePhrases.length) {
          print('[_initVoiceprint] ğŸ‰ å£°çº¹æ³¨å†Œå…¨éƒ¨å®Œæˆï¼');
          FlutterForegroundTask.sendDataToMain({'status': 'completed'});
          _isNeedVoiceprintInit = false;
        } else {
          FlutterForegroundTask.sendDataToMain({'status': 'success', 'step': currentStep});
        }
      } catch (e) {
        print('[_initVoiceprint] âŒ å£°çº¹æ³¨å†Œè¿‡ç¨‹å‡ºé”™: $e');
        FlutterForegroundTask.sendDataToMain({'status': 'error', 'message': 'å£°çº¹æ³¨å†Œå¤±è´¥: $e'});
      }
    } else {
      print('[_initVoiceprint] âŒ æ–‡æœ¬ä¸åŒ¹é…ï¼Œéœ€è¦é‡è¯•');
      FlutterForegroundTask.sendDataToMain({'status': 'failure', 'similarity': similarity});
    }
  }

  // å¯åŠ¨å£°çº¹æ³¨å†Œæ¨¡å¼
  void _startVoiceprint(){
    print('[_startVoiceprint] ğŸ—£ï¸ Starting voiceprint initialization...');

    if (_cloudTts.isPlaying) {
      print('[_startVoiceprint] ğŸ”‡ Stopping cloud TTS...');
      _cloudTts.stop();
    }

    print('[_startVoiceprint] ğŸ”„ Resetting states...');
    _inDialogMode = false;

    print('[_startVoiceprint] ğŸ§¹ Clearing existing speakers...');
    _manager?.allSpeakerNames.forEach((name) {
      _manager?.remove(name);
    });

    currentStep = 0;
    _isNeedVoiceprintInit = true;

    // print('[_startVoiceprint] âœ… Voiceprint mode enabled: _isNeedVoiceprintInit = $_isNeedVoiceprintInit');
    // print('[_startVoiceprint] ğŸ“Š Current state: _onRecording = $_onRecording, _onMicrophone = $_onMicrophone');
  }

  // åœæ­¢å½•éŸ³å¹¶é‡Šæ”¾èµ„æº
  Future<void> _stopRecord() async {
    if (_recordStream != null) {
      await _record.stop();
      await _record.dispose();
      _recordStream = null;
    }

    _recordSub?.cancel();
    _currentSubscription?.cancel();
    _vad?.free();

    // é‡Šæ”¾æµå¼ASRæœåŠ¡èµ„æº
    _streamingAsr.dispose();

    _manager?.free();
    _extractor?.free();

    _isInitialized = false;

    FlutterForegroundTask.saveData(key: 'isRecording', value: false);
    FlutterForegroundTask.updateService(
        notificationText: 'Tap to return to the app'
    );
  }

  // åœæ­¢éº¦å…‹é£å½•éŸ³
  Future<void> _stopMicrophone() async {
    if (!_onMicrophone) return;
    if (_recordStream != null) {
      await _record.stop();
      await _record.dispose();
      _recordStream = null;
      _onMicrophone = false;
    }
  }

  // è·å–è¯´è¯äººembeddingï¼ˆå£°çº¹ç‰¹å¾ï¼‰
  Float32List getSpeakerEmbedding(samplesBuffer) {
    // æ£€æŸ¥ _extractor æ˜¯å¦å¯ç”¨
    if (_extractor == null) {
      print('[getSpeakerEmbedding] âš ï¸ Speaker extractor not available, returning dummy embedding');
      // è¿”å›ä¸€ä¸ªè™šæ‹Ÿçš„embeddingï¼Œé¿å…nullé”™è¯¯
      return Float32List(512); // åˆ›å»ºä¸€ä¸ª512ç»´çš„é›¶å‘é‡
    }

    try {
      final speakerStream = _extractor!.createStream();
      speakerStream.acceptWaveform(samples: Float32List.fromList(samplesBuffer), sampleRate: 16000);
      final embedding = _extractor!.compute(speakerStream);
      speakerStream.free();

      // éªŒè¯embeddingè´¨é‡
      if (embedding.isEmpty) {
        print('[getSpeakerEmbedding] âš ï¸ Empty embedding returned');
        return Float32List(512);
      }

      print('[getSpeakerEmbedding] âœ… æˆåŠŸæå–å£°çº¹ç‰¹å¾ï¼Œç»´åº¦: ${embedding.length}');
      print('[getSpeakerEmbedding] ğŸ“Š Embedding preview: ${embedding.length > 10 ? embedding.sublist(0, 10) : embedding}');

      return embedding;
    } catch (e) {
      print('[getSpeakerEmbedding] âŒ å£°çº¹æå–å¤±è´¥: $e');
      return Float32List(512);
    }
  }

  // æ”¹è¿›çš„è¯´è¯äººè¯†åˆ«é€»è¾‘ - ä¸“æ³¨äºåŒºåˆ†"æˆ‘"å’Œ"åˆ«äºº"
  String _identifySpeaker(Float32List embedding) {
    print('[_identifySpeaker] ğŸ” å¼€å§‹è¯´è¯äººè¯†åˆ«...');

    // æ£€æŸ¥æ˜¯å¦æœ‰ï¿½ï¿½æ³¨å†Œçš„ç”¨æˆ·å£°çº¹
    final userSpeakers = _objectBoxService.getUserSpeaker();
    if (userSpeakers == null || userSpeakers.isEmpty) {
      print('[_identifySpeaker] âš ï¸ æ²¡æœ‰æ³¨å†Œçš„ç”¨æˆ·å£°çº¹ï¼Œè¿”å› others');
      return 'others';
    }

    // è·å–ä¸»ç”¨æˆ·çš„å£°çº¹ï¼ˆå‡è®¾ç¬¬ä¸€ä¸ªæ˜¯ä¸»ç”¨æˆ·ï¼‰
    var mainUser = userSpeakers.firstWhere(
            (speaker) => speaker.name == 'user' || speaker.name == 'main_user',
        orElse: () => userSpeakers.first
    );

    if (mainUser.embedding == null || mainUser.embedding!.isEmpty) {
      print('[_identifySpeaker] âš ï¸ ä¸»ç”¨æˆ·å£°çº¹ä¸ºç©ºï¼Œè¿”å› others');
      return 'others';
    }

    // è®¡ç®—ä¸ä¸»ç”¨æˆ·çš„ç›¸ä¼¼åº¦
    final userEmbedding = Float32List.fromList(mainUser.embedding!);
    final similarity = _improvedCosineSimilarity(embedding, userEmbedding);

    print('[_identifySpeaker] ğŸ“Š ä¸ä¸»ç”¨æˆ·ç›¸ï¿½ï¿½åº¦: $similarity');

    // åŠ¨æ€é˜ˆå€¼è°ƒæ•´ - æ ¹æ®å†å²æ•°æ®è°ƒæ•´
    double threshold = _calculateDynamicThreshold();
    print('[_identifySpeaker] ğŸ¯ ä½¿ç”¨é˜ˆå€¼: $threshold');

    if (similarity >= threshold) {
      print('[_identifySpeaker] âœ… è¯†åˆ«ä¸ºç”¨æˆ·æœ¬äºº');
      _updateSpeakerHistory(true, similarity);
      return 'user';
    } else {
      print('[_identifySpeaker] âŒ è¯†åˆ«ä¸ºå…¶ä»–äºº');
      _updateSpeakerHistory(false, similarity);
      return 'others';
    }
  }

  // æ”¹è¿›çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¢åŠ æ•°å€¼ç¨³å®šæ€§
  double _improvedCosineSimilarity(Float32List a, Float32List b) {
    if (a.length != b.length) {
      print('[_improvedCosineSimilarity] âš ï¸ å‘é‡é•¿åº¦ä¸åŒ¹é…: ${a.length} vs ${b.length}');
      return -1.0;
    }

    double dot = 0.0;
    double normA = 0.0;
    double normB = 0.0;

    for (int i = 0; i < a.length; i++) {
      dot += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }

    // æ·»åŠ å°çš„epsiloné¿å…é™¤é›¶
    const double epsilon = 1e-10;
    normA = math.sqrt(normA + epsilon);
    normB = math.sqrt(normB + epsilon);

    if (normA < epsilon || normB < epsilon) {
      print('[_improvedCosineSimilarity] âš ï¸ å‘é‡èŒƒæ•°è¿‡å°');
      return -1.0;
    }

    double similarity = dot / (normA * normB);

    // ç¡®ä¿ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´å†…
    return similarity.clamp(-1.0, 1.0);
  }

  // åŠ¨æ€é˜ˆå€¼è®¡ç®— - æ ¹æ®å†å²è¯†åˆ«æ•°æ®è°ƒæ•´
  double _calculateDynamicThreshold() {
    // è·å–å†å²è¯†åˆ«ï¿½ï¿½æ®
    final recentRecords = _objectBoxService.getRecentRecords(limit: 50);
    if (recentRecords == null || recentRecords.isEmpty) {
      return 0.65; // é»˜è®¤é˜ˆå€¼
    }

    // ç»Ÿè®¡ç”¨æˆ·å’Œå…¶ä»–äººçš„ï¿½ï¿½å½•æ¯”ä¾‹
    int userCount = recentRecords.where((r) => r.role == 'user').length;
    int othersCount = recentRecords.where((r) => r.role == 'others').length;

    double userRatio = userCount / (userCount + othersCount);

    // æ ¹æ®ä½¿ç”¨æ¨¡å¼è°ƒæ•´é˜ˆå€¼
    if (userRatio > 0.8) {
      // ä¸»è¦æ˜¯ç”¨æˆ·è‡ªå·±ä½¿ç”¨ï¼Œæé«˜é˜ˆå€¼é¿å…è¯¯è¯†åˆ«
      return 0.75;
    } else if (userRatio < 0.3) {
      // å¤šäººç¯å¢ƒï¼Œé™ä½é˜ˆå€¼ç¡®ä¿èƒ½è¯†åˆ«åˆ°ç”¨æˆ·
      return 0.55;
    } else {
      // å¹³è¡¡ç¯å¢ƒï¼Œä½¿ç”¨ä¸­ç­‰é˜ˆå€¼
      return 0.65;
    }
  }

  void _updateSpeakerHistory(bool isUser, double similarity) {
    if (isUser) {
      _userSimilarityHistory.add(similarity);
      if (_userSimilarityHistory.length > 20) {
        _userSimilarityHistory.removeAt(0);
      }
    } else {
      _othersSimilarityHistory.add(similarity);
      if (_othersSimilarityHistory.length > 20) {
        _othersSimilarityHistory.removeAt(0);
      }
    }
  }

  // å£°çº¹è´¨é‡è¯„ä¼°
  bool _isEmbeddingQualityGood(Float32List embedding) {
    if (embedding.isEmpty) return false;

    // æ£€æŸ¥æ˜¯å¦å…¨ä¸ºé›¶æˆ–æ¥è¿‘é›¶
    double magnitude = 0.0;
    for (double value in embedding) {
      magnitude += value * value;
    }
    magnitude = math.sqrt(magnitude);

    // å¦‚æœå‘é‡å¹…åº¦å¤ªå°ï¼Œè®¤ä¸ºè´¨é‡ä¸å¥½
    if (magnitude < 0.01) {
      print('[_isEmbeddingQualityGood] âš ï¸ å£°çº¹å‘é‡å¹…åº¦è¿‡å°: $magnitude');
      return false;
    }

    // æ£€æŸ¥æ–¹å·®ï¼Œé¿å…å‘é‡è¿‡äºå¹³å¦
    double mean = embedding.reduce((a, b) => a + b) / embedding.length;
    double variance = 0.0;
    for (double value in embedding) {
      variance += (value - mean) * (value - mean);
    }
    variance /= embedding.length;

    if (variance < 0.001) {
      print('[_isEmbeddingQualityGood] âš ï¸ å£°çº¹å‘é‡æ–¹å·®è¿‡å°: $variance');
      return false;
    }

    return true;
  }

  void _checkAndSummarizeDialogue() {
    final now = DateTime.now().millisecondsSinceEpoch;
    print('[è‡ªåŠ¨æ€»ç»“] æ£€æŸ¥æ¡ä»¶ï¼šå½“å‰ç´¯è®¡å­—æ•°=$_currentDialogueCharCountï¼Œæœ€åè¯´è¯æ—¶é—´=$_lastSpeechTimestampï¼Œå½“å‰æ—¶é—´=$now');
    // 1. è¶…è¿‡æœ€å¤§å­—æ•°å¼ºåˆ¶åˆ†æ®µ
    if (_currentDialogueCharCount >= maxCharLimit) {
      print('[è‡ªåŠ¨æ€»ç»“] è¶…è¿‡æœ€å¤§å­—æ•°ï¼Œå¼ºåˆ¶åˆ†æ®µæ€»ç»“...');
      DialogueSummary.start(
        startTime: _currentDialogueStartTime,
        onSummaryCallback: _handleSummaryGenerated,
      );
      _currentDialogueCharCount = 0;
      _lastSpeechTimestamp = 0;
      _currentDialogueStartTime = null;
      return;
    }
    // 2. æ­£å¸¸å¯¹è¯åˆ†æ®µé€»è¾‘
    if (_currentDialogueCharCount >= minCharLimit &&
        _lastSpeechTimestamp > 0 &&
        now - _lastSpeechTimestamp > 0.25 * 60 * 1000) {
      print('[è‡ªåŠ¨æ€»ç»“] æ»¡è¶³æ¡ä»¶ï¼Œå¼€å§‹è‡ªåŠ¨æ•´ç†å¯¹è¯å†…å®¹...');
      DialogueSummary.start(
        startTime: _currentDialogueStartTime,
        onSummaryCallback: _handleSummaryGenerated,
      );
      _currentDialogueCharCount = 0;
      _lastSpeechTimestamp = 0;
      _currentDialogueStartTime = null;
    } else {
      print('[è‡ªåŠ¨æ€»ç»“] æœªæ»¡è¶³è‡ªåŠ¨æ€»ç»“æ¡ä»¶');
    }
  }

  // ğŸ”¥ æ–°å¢ï¼šå¤„ç†æ‘˜è¦ç”Ÿæˆå®Œæˆçš„å›è°ƒ
  void _handleSummaryGenerated(List<SummaryEntity> summaries) {
    print('[ASR] ğŸ“‹ æ”¶åˆ°æ‘˜è¦ç”Ÿæˆå®Œæˆé€šçŸ¥ï¼Œæ‘˜è¦æ•°é‡: ${summaries.length}');

    if (summaries.isEmpty) return;

    try {
      // æ„å»ºæ‘˜è¦æ˜¾ç¤ºå†…å®¹
      String summaryContent = _formatSummaryForDisplay(summaries);

      // é€šè¿‡FlutterForegroundTaskå‘é€æ‘˜è¦æ¶ˆæ¯åˆ°ä¸»ç•Œé¢
      FlutterForegroundTask.sendDataToMain({
        'text': summaryContent,
        'isEndpoint': true,
        'speaker': 'system',
        'isSummary': true, // æ ‡è¯†è¿™æ˜¯æ‘˜è¦æ¶ˆæ¯
      });

      print('[ASR] âœ… æ‘˜è¦æ¶ˆæ¯å·²å‘é€åˆ°ä¸»ç•Œé¢');

    } catch (e) {
      print('[ASR] âŒ å¤„ç†æ‘˜è¦æ˜¾ç¤ºæ—¶å‡ºé”™: $e');
    }
  }

  // ğŸ”¥ æ–°å¢ï¼šæ ¼å¼åŒ–æ‘˜è¦å†…å®¹ç”¨äºæ˜¾ç¤º
  String _formatSummaryForDisplay(List<SummaryEntity> summaries) {
    StringBuffer buffer = StringBuffer();
    buffer.writeln('ğŸ“‹ **å¯¹è¯æ€»ç»“**');
    buffer.writeln('');

    for (int i = 0; i < summaries.length; i++) {
      SummaryEntity summary = summaries[i];

      // æ ¼å¼åŒ–æ—¶é—´
      String startTimeStr = DateFormat('HH:mm').format(
        DateTime.fromMillisecondsSinceEpoch(summary.startTime)
      );
      String endTimeStr = DateFormat('HH:mm').format(
        DateTime.fromMillisecondsSinceEpoch(summary.endTime)
      );

      buffer.writeln('**${i + 1}. ${summary.subject}** (${startTimeStr}-${endTimeStr})');
      buffer.writeln(summary.content);

      if (i < summaries.length - 1) {
        buffer.writeln('');
      }
    }

    return buffer.toString();
  }
}

// // æ–°ç‰ˆæœ¬
// Future<sherpa_onnx.VoiceActivityDetector> initVad() async =>
//     sherpa_onnx.VoiceActivityDetector(
//       config: sherpa_onnx.VadModelConfig(
//         sileroVad: sherpa_onnx.SileroVadModelConfig(
//           model: await copyAssetFile('assets/silero_vad.onnx'),
//           threshold: 0.5,
//           minSilenceDuration: 0.25,
//           minSpeechDuration: 0.5,
//           maxSpeechDuration: 5.0,
//           windowSize: 512,
//         ),
//         sampleRate: 16000,
//         numThreads: 1,
//         provider: "cpu",
//         debug: true,
//       ),
//       bufferSizeInSeconds: 2.0,
//     );
// æ—§ç‰ˆæœ¬
Future<sherpa_onnx.VoiceActivityDetector> initVad() async =>
    sherpa_onnx.VoiceActivityDetector(
      config: sherpa_onnx.VadModelConfig(
        sileroVad: sherpa_onnx.SileroVadModelConfig(
          model: await copyAssetFile('assets/silero_vad.onnx'),
          minSilenceDuration: 0.25,
          minSpeechDuration: 0.5,
          maxSpeechDuration: 5.0,
        ),
        numThreads: 1,
        debug: true,
      ),
      bufferSizeInSeconds: 2.0,
    );

Future<List<List<double>>> loadMatrixFromJson(String assetPath, int rows, int cols) async {
  // ä»jsonåŠ è½½çŸ©é˜µï¼ˆList<List<double>>ï¼‰
  String jsonString = await rootBundle.loadString(assetPath);
  List<dynamic> jsonData = jsonDecode(jsonString);
  print('Loaded JSON data type: ${jsonData.runtimeType}');
  print('Number of elements in jsonData: ${jsonData.length}');
  // Check if jsonData is empty
  if (jsonData.isEmpty) {
    print('jsonData is empty!');
    return [];
  }
  // Check if the length of jsonData matches the expected size
  if (jsonData.length != rows * cols) {
    print('Warning: jsonData length (${jsonData.length}) does not match expected size ($rows * $cols).');
    return [];
  }
  List<List<double>> matrix = List.generate(rows, (i) {
    return List.generate(cols, (j) {
      return jsonData[i * cols + j].toDouble();
    });
  });
  return matrix;
}

Future<Matrix> loadRealMatrixFromJson(String assetPath, int rows, int cols) async {
  // ä»jsonåŠ è½½çŸ©é˜µï¼ˆMatrixå¯¹è±¡ï¼‰
  String jsonString = await rootBundle.loadString(assetPath);
  List<dynamic> jsonData = jsonDecode(jsonString);
  debugPrint('Loaded JSON data type: ${jsonData.runtimeType}');
  debugPrint('Number of elements in jsonData: ${jsonData.length}');

  Matrix matrix = Matrix.fill(rows, cols, 0.0);
  if (jsonData.isEmpty) { // Check if jsonData is empty
    debugPrint('jsonData is empty!');
  } else if (jsonData.length != rows * cols) { // Check if the length of jsonData matches the expected size
    debugPrint('Warning: jsonData length (${jsonData.length}) does not match expected size ($rows * $cols).');
  } else {
    for (int i = 0; i < rows; i++) {
      for (int j = 0; j < cols; j++) {
        matrix[i][j] = jsonData[i * cols + j].toDouble();
      }
    }
  }

  return matrix;
}

Future<Float32List> _addSilencePadding(Float32List samples) async {
  // ä¸ºéŸ³é¢‘æ·»åŠ é™éŸ³padding
  int totalLength = silence.length * 2 + samples.length;

  Float32List paddedSamples = Float32List(totalLength);

  paddedSamples.setAll(silence.length, samples);

  return paddedSamples;
}

// åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ä½™å¼¦ç›¸ä¼¼åº¦å‡½æ•°

double _cosineSimilarity(Float32List a, Float32List b) {
  if (a.length != b.length) return -1.0;
  double dot = 0.0;
  double normA = 0.0;
  double normB = 0.0;
  for (int i = 0; i < a.length; i++) {
    dot += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }
  if (normA == 0 || normB == 0) return -1.0;
  return dot / (math.sqrt(normA) * math.sqrt(normB)); // â† ä½¿ç”¨math.sqrt
}











































































